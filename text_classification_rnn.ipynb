{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/main/text_classification_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SN5USFEIIK3"
      },
      "source": [
        "# Text Classification using RNN\n",
        "In this lab exercise, we will learn to use LSTM (an RNN variant) to train a model to classify a piece of text as expressing positive sentiment or negative sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZUQErGewZxE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RutaI-Tpev3T"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFctV8-JZOc"
      },
      "source": [
        "### Download the IMDb Dataset\n",
        "You will use the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/). You will train a sentiment classifier model on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPO4_UmfF0KH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b9cd08-bfb6-4714-bf54-983b691fe766"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imdbEr.txt', 'imdb.vocab', 'train', 'README', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY6yROZNKvbd"
      },
      "source": [
        "Take a look at the `train/` directory. It has `pos` and `neg` folders with movie reviews labelled as positive and negative respectively. You will use reviews from `pos` and `neg` folders to train a binary classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iOHJGN6SDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9072ee-48a8-4406-ebcb-d16cc62f45e9"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['urls_neg.txt',\n",
              " 'urls_unsup.txt',\n",
              " 'labeledBow.feat',\n",
              " 'unsup',\n",
              " 'unsupBow.feat',\n",
              " 'urls_pos.txt',\n",
              " 'neg',\n",
              " 'pos']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O59BdioK8jY"
      },
      "source": [
        "The `train` directory also has additional folders which should be removed before creating training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Vfi9oWMSh-"
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFoJjiEyJz9u"
      },
      "source": [
        "Next, create a `tf.data.Dataset` using `tf.keras.preprocessing.text_dataset_from_directory`. You can read more about this utility from the [api documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory). \n",
        "\n",
        "Use the `train` directory to create both train and validation datasets with a split of 20% for validation. Also note that here we use a smaller batch size of 128, as our model now is more complex, and will use up some significant memory, leaving little room for larger batch size.\n",
        "\n",
        "***Important note***\n",
        "\n",
        "Note: When using the `validation_split` and `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation and training splits have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItYD3TLkCOP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5d502d-c79c-4e11-90cd-6a6e13f26330"
      },
      "source": [
        "batch_size = 128\n",
        "seed = 123\n",
        "train_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, \n",
        "    subset='training', seed=seed)\n",
        "val_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, \n",
        "    subset='validation', seed=seed)\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHa6cq0-Ym0g"
      },
      "source": [
        "Take a look at a few movie reviews and their labels `(1: positive, 0: negative)` from the train dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTCbSkvkYmTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b9b8aa-1e36-4a06-ad33-d2b566fb5473"
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "    for i in range(3):\n",
        "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 b\"I have watched this movie well over 100-200 times, and I love it each and every time I watched it. Yes, it can be very corny but it is also very funny and enjoyable. The camp shown in the movie is a real camp that I actually attended for 7 years and is portrayed as camp really is, a great place to spend the summer. Everyone who has ever gone to camp, wanted to go to camp, or has sent a child to camp should see this movie because it'll bring back wonderful memories for you and for your kids.\"\n",
            "1 b'This movie is SOOOO funny!!! The acting is WONDERFUL, the Ramones are sexy, the jokes are subtle, and the plot is just what every high schooler dreams of doing to his/her school. I absolutely loved the soundtrack as well as the carefully placed cynicism. If you like monty python, You will love this film. This movie is a tad bit \"grease\"esk (without all the annoying songs). The songs that are sung are likable; you might even find yourself singing these songs once the movie is through. This musical ranks number two in musicals to me (second next to the blues brothers). But please, do not think of it as a musical per say; seeing as how the songs are so likable, it is hard to tell a carefully choreographed scene is taking place. I think of this movie as more of a comedy with undertones of romance. You will be reminded of what it was like to be a rebellious teenager; needless to say, you will be reminiscing of your old high school days after seeing this film. Highly recommended for both the family (since it is a very youthful but also for adults since there are many jokes that are funnier with age and experience.'\n",
            "1 b\"I saw Insomniac's Nightmare not to long ago for the first time and I have to say, I really found it to be quite good. If you are a fan of Dominic Monaghan you will love it. The hole movie takes place inside his mind -or does it? The acting from everyone else is a little rushed and shaky and some of the scenes could be cut down but it works out in the end. The extras on the DVD are just as great as the film, if not greater for those Dom fans. It has tons of candid moments from the set, outtakes and a great interview with the director. Anyone who has gone through making an independent film will love to watch Tess (the director), Dom and everyone else on the very small close personal set try to bang out this little trippy creepy film. It was pretty enjoyable and I'm glad to have it in my collection.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHV2pchDhzDn"
      },
      "source": [
        "### Configure the dataset for performance\n",
        "\n",
        "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
        "\n",
        "`.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
        "\n",
        "`.prefetch()` overlaps data preprocessing and model execution while training. \n",
        "\n",
        "You can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz6k1IW7h1TO"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGicgV5qT0wh"
      },
      "source": [
        "## Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6NZSqIIoU0Y"
      },
      "source": [
        "Next, define the dataset preprocessing steps required for your sentiment classification model. Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews. \n",
        "\n",
        "TextVectorization layer is a text tokenizer which breaks up the text into words (it is similar to Keras Tokenizer but implemented as a layer). You can read more about TextVectorization layer [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n",
        "\n",
        "There are two ways to use TextVectorization: \n",
        "1. as part of the `tf.data` pipeline\n",
        "2. as part of the model (i.e. as a layer in the model)\n",
        "\n",
        "If you are doing training on GPU, it is better to use option 1 as it allows you to do asynchronous preprocessing of your data on CPU (because text vectorization does not use GPU), while GPU runs the model on one batch of data.This will lead to better training throughput. \n",
        "\n",
        "However, if you are exporting the model for inference in the production environment, you want to package the TextVectorization layer as part of the model, to make entire model self-contained without having to deploy additional preprocessing codes.\n",
        "\n",
        "In the code below, we will use option 1 for pre-processing text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MlsXzo-ZlfK"
      },
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 200\n",
        "# Use the text vectorization layer to normalize, split, and map strings to \n",
        "# integers.\n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE, \n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
        ")\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsxTnJSv2kF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50f12ab-49ab-4960-c869-b85c6e7d295c"
      },
      "source": [
        "print(len(vectorize_layer.get_vocabulary()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WFdR5980Hvj"
      },
      "source": [
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (vectorize_layer(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (vectorize_layer(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (vectorize_layer(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS4lFYwug6qN",
        "outputId": "f9ceb9a5-f8e7-41ca-e437-a4bc9add8607"
      },
      "source": [
        "for x, y in int_train_ds.take(1):\n",
        "    print(x[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[  13   13  143    8   25 1944    2  164  131   68  464    6 2199   25\n",
            "   83 3413   19   25  225    1    9  107 2836  294    2  164  131   44\n",
            " 2789  293  474  106  892    3   29  247    3 1966   25  511    3  579\n",
            "    6 2199    3   26  474  507    2  793    7 1059   16  634  482    3\n",
            " 2916    8    2 1385    5  634    1   13   29  223  103   12    2  293\n",
            " 5856   24 1541 1130   21    4 2858 1121   19   58   35   24 3973  804\n",
            " 6163  165    6 1155    3 2953    8    2 1011 3896   13  110   81   71\n",
            "   66    4    1  541  193   98    1  646    3    2  106  990  110   81\n",
            "   71   66   98    1  541  193   98    1  919    3    2   29  793   32\n",
            "    1  391   24 5945  193   60  193    2  990    3    1   13    2 4229\n",
            "  168    5 9482   14    2 7988  151    2  180 5208    1   59   26 3119\n",
            "    5 4554  484    3    4  550  372    5    1  484   32  124   89   19\n",
            "   58   28   60  464   25  793   45   18 7988    7  123 3367    5 3472\n",
            "   92 3825   76 1282 3976   23    6   58  128    0    0    0    0    0\n",
            "    0    0    0    0], shape=(200,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI9_wLIiWO8Z"
      },
      "source": [
        "## Create a classification model\n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/it3103/bidirectionalRNN.png\"/>\n",
        "\n",
        "Above is a diagram of the model. \n",
        "\n",
        "1. This model can be built as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the vectorization layer, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the vectorization layer is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHLcFtn5Wsqj"
      },
      "source": [
        "EMBEDDING_DIM=128\n",
        "\n",
        "inputs = keras.Input(shape=(None,))\n",
        "embedded = keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
        "                                  output_dim=EMBEDDING_DIM,\n",
        "                                  name='embedding')(inputs)\n",
        "x = keras.layers.Masking(mask_value=0.0)(embedded)\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(embedded)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "x = keras.layers.Dense(64)(x)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjLNgKO7W2fe"
      },
      "source": [
        "## Compile and train the model\n",
        "\n",
        "We will use the model_checkpoint_callback to save our best checkpoint in terms of validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Hg3IHFt4Px"
      },
      "source": [
        "def save_best_model(checkpoint_path): \n",
        "\n",
        "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "    \n",
        "    return model_checkpoint_callback"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OrKAKAKIbuH"
      },
      "source": [
        "Compile and train the model using the `Adam` optimizer and `BinaryCrossentropy` loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCUgdP69Wzix"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mQehiQyv8rP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88b32a9-6fa6-44a9-f5ab-b6009d05ed49"
      },
      "source": [
        "model.fit(\n",
        "    int_train_ds, \n",
        "    validation_data=int_val_ds,\n",
        "    epochs=3, \n",
        "    callbacks=[save_best_model('best_checkpoint_1')])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "157/157 [==============================] - 19s 91ms/step - loss: 0.4976 - accuracy: 0.7354 - val_loss: 0.3572 - val_accuracy: 0.8518\n",
            "Epoch 2/3\n",
            "157/157 [==============================] - 13s 83ms/step - loss: 0.2834 - accuracy: 0.8881 - val_loss: 0.3671 - val_accuracy: 0.8514\n",
            "Epoch 3/3\n",
            "157/157 [==============================] - 12s 76ms/step - loss: 0.2243 - accuracy: 0.9151 - val_loss: 0.3705 - val_accuracy: 0.8358\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdf916db050>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wYnVedSPfmX"
      },
      "source": [
        "The model reaches a validation accuracy of around 85% after 1 epoch of training.\n",
        "\n",
        "Note: Your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTJMGvvR6mTs"
      },
      "source": [
        "Let's evaluate the model on our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk-tEK086qko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc43331-eb00-4eed-c9d6-27b682e22bf0"
      },
      "source": [
        "model.load_weights(\"best_checkpoint_1\")\n",
        "model.evaluate(int_test_ds)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196/196 [==============================] - 11s 52ms/step - loss: 0.3933 - accuracy: 0.8333\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3932574689388275, 0.8332800269126892]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzXFaQSIkdjJ"
      },
      "source": [
        "## Prepare Model for Deployment \n",
        "\n",
        "As mentioned earlier, it is better to package the TextVectorization layer as part of the model for ease of deployment, so that we can run the raw text directly through the model during inference.\n",
        "\n",
        "In the code below, we declare a Input layer that takes in a string (shape=(1,)), and we add the Text Vectorization layer, and then stick them to our previous model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyn9rYk042RO"
      },
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = vectorize_layer(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzSEgryN1_1B"
      },
      "source": [
        "\n",
        "Let's go ahead and save our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKnKLmU_ksqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086ab437-a32a-4b2c-ed73-340b6dcf9d9e"
      },
      "source": [
        "inference_model.save('sentiment_model')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: sentiment_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: sentiment_model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fdf91704350> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fdf8ea39890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtek2H0A2TWK"
      },
      "source": [
        "Now let us put our model in use!!  We will first load our saved model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1DJOzal2yQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75639b1-3440-4063-8d16-4f233b855707"
      },
      "source": [
        "loaded_model = keras.models.load_model('sentiment_model')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edrL9YQ03Tqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55cb6dfe-261b-45bc-9882-2ff8781c33d5"
      },
      "source": [
        "loaded_model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " model (Functional)          (None, 1)                 1387137   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,387,137\n",
            "Trainable params: 1,387,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLaU3AKb3T7v"
      },
      "source": [
        "Run the following cell and type in your own text at the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yBr_A1W2h6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56173cd3-67b9-4f8b-bb0a-222a5ba0b5e8"
      },
      "source": [
        "text = input(\"Write your review here:\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write your review here:I am sad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR-ZsF345rSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d28fac8-a8aa-43bf-89bf-160921a9cf8e"
      },
      "source": [
        "pred = loaded_model.predict([text])[0]\n",
        "if pred >= 0.5: \n",
        "    print('positive sentiment')\n",
        "else:\n",
        "    print('negative sentiment')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c570Kdh-kmYZ"
      },
      "source": [
        "## Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/it3103/layered_bidirectional.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29uXYt5gkuW8"
      },
      "source": [
        "inputs = keras.Input(shape=(None,))\n",
        "embedded = keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
        "                                  output_dim=EMBEDDING_DIM,\n",
        "                                  name='embedding')(inputs)\n",
        "x = keras.layers.Masking(mask_value=0.0)(embedded)\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(64,  return_sequences=True))(x)\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(32))(x)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "x = keras.layers.Dense(64, activation='relu')(x)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYIpOKjFk2SU"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YluqzM-kk29d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307f342d-581d-488a-cb31-dc19a76ae31b"
      },
      "source": [
        "model.fit(int_train_ds, \n",
        "          epochs=3,\n",
        "          validation_data=int_val_ds,\n",
        ")\n",
        "\n",
        "#callbacks=[save_best_model('best_checkpoint_2')])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "157/157 [==============================] - 50s 208ms/step - loss: 0.6927 - accuracy: 0.5192 - val_loss: 0.6919 - val_accuracy: 0.5122\n",
            "Epoch 2/3\n",
            "157/157 [==============================] - 25s 157ms/step - loss: 0.6052 - accuracy: 0.6781 - val_loss: 0.4214 - val_accuracy: 0.8228\n",
            "Epoch 3/3\n",
            "157/157 [==============================] - 23s 150ms/step - loss: 0.3419 - accuracy: 0.8663 - val_loss: 0.3285 - val_accuracy: 0.8636\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdedd9928d0>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoGvifEkr4YJ",
        "outputId": "470fcf88-a44d-44fb-eada-4d6f5f0e839e"
      },
      "source": [
        "model.load_weights(\"best_checkpoint_2\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fdee4de7350>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tDbhq4ek9lq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53fcc48b-4eb4-43a1-ab0d-1de81d3bb944"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(int_test_ds)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196/196 [==============================] - 14s 72ms/step - loss: 0.3502 - accuracy: 0.8510\n",
            "Test Loss: 0.35017356276512146\n",
            "Test Accuracy: 0.8509600162506104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pimDxqhrem-"
      },
      "source": [
        "As before, we prepare the model for deployment by adding in vectorization layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zGairo5qbbW"
      },
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = vectorize_layer(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)\n",
        "inference_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmt-SB9byQaK",
        "outputId": "8bab40cb-d60e-40b9-a5e7-2a4598b21692"
      },
      "source": [
        "inference_model.evaluate(test_ds)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196/196 [==============================] - 25s 75ms/step - loss: 0.3502 - accuracy: 0.8510\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.35017356276512146, 0.8509600162506104]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ1kbFOuyhcV",
        "outputId": "6beb8ec3-f435-490b-fdd2-194ed554f7d3"
      },
      "source": [
        "for x, y in test_ds.take(1):\n",
        "    print(x[8], y[8])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'I understand \"Checking Out\" will likely be released in Theatres in the USA in June 2006, and on DVD in November 2006. My recommendation is to not miss \"Checking Out\"!! This Comedy Film will entertain everyone, who will all relate to the characters, family relationships and multiple social issues that are portrayed. \"Checking Out\" will make you laugh throughout with quick fire clever humor built into almost every line, and may make you poignantly cry in a touching positive way as well. <br /><br />The subject of suicide is dealt with in a comical way, that at the same time may help people considering it understand the impacts this act may have on those their life has touched and on those who love them. Lets hope that \"Checking Out\" can have a positive impact and help prevent those considering suicide from acting it out, especially in the 16-25 age group that has the highest rate of suicide in the USA. <br /><br />The script is wonderfully written, perfectly casted, and loaded with synchronicity and meaningful flashbacks in time, whose significance become more apparent throughout the film and especially at the ending. I believe the script is worthy of an Academy Award Nomination for \"Best Screenplay, Play to a Movie\" ( The Phoenix Film Festival honored \"Checking Out\" with a \"Best Screenplay Award\" ).<br /><br />The cast is loaded with great actors that out do themselves, and have a long track record of great performances, acting award nominations and wins. I feel Peter Falk\\'s performance in \"Checking Out\" is worthy of an Academy Award Nomination, and is the most challenging and wide ranging of his career. Laura San Giacomo\\'s performance and chemistry with Peter Falk as her father is masterful, and was recognized at the Palm Beach International Film Festival with a \"Best Actress Award.\"<br /><br />Even the teenage characters in the movie shine and are played by young acting phenoms Dan Byrd ( Movies: A Cinderalla Story with Hilary Duff; 3 Young Artist Award nominations, Won 1 ) and Mary Elizabeth Winstead ( TV: Monster Island, Wolf Lake, Passions, Touched by an Angel; 2 Young Star Nominations ).<br /><br />Director Jeff Hare and Producer Mark Lane wonderfully develop the characters, their interrelationships, and the story line of this entertaining, enjoyable yet complex script.<br /><br />The film editing keeps the pace of the film moving quickly, only appropriately slowing in the poignant scenes, so that the audience will never loose interest from the beginning to the end.<br /><br />Don\\'t miss this film that you can take the whole family to see and all will enjoy it.', shape=(), dtype=string) tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUf8hjoYyY9w",
        "outputId": "2eb3d855-c1a2-4963-d91e-3f102a86f768"
      },
      "source": [
        "preds[:10]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.30757847],\n",
              "       [0.04423553],\n",
              "       [0.67033714],\n",
              "       [0.9336638 ],\n",
              "       [0.15066113],\n",
              "       [0.9383219 ],\n",
              "       [0.97367924],\n",
              "       [0.97131485],\n",
              "       [0.9494515 ],\n",
              "       [0.19111809]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU60HlRbz3Ov"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrUj22o7lDhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8616456e-8386-4f46-82b4-b8805516f600"
      },
      "source": [
        "sample_text = \"As a recreational golfer with some knowledge of the sport's history, I was pleased with Disney's sensitivity to the issues of class in golf in the early twentieth century. The movie depicted well the psychological battles that Harry Vardon fought within himself, from his childhood trauma of being evicted to his own inability to break that glass ceiling that prevents him from being accepted as an equal in English golf society.\"\n",
        "sample_text = \"This movie has to be one of the worst I have seen. \"\n",
        "pred = inference_model.predict([sample_text])[0]\n",
        "print(pred)\n",
        "if pred >= 0.5: \n",
        "    print(f'positive sentiment: {pred}')\n",
        "else:\n",
        "    print(f'negative sentiment: {pred}')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1059983]\n",
            "negative sentiment: [0.1059983]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrZ8iX8AqtF2"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "Experiment with any of following to see if you get better or worse validation accuracy.\n",
        "\n",
        "1. Add in Dropout layer\n",
        "2. Increase vocabulary size \n",
        "2. Increase Embedding dimensions \n",
        "4. Use uni-directional LSTM instead of bidirectional\n"
      ]
    }
  ]
}