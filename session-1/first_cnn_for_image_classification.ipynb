{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/main/session-1/first_cnn_for_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6gHiH-I7uFa"
   },
   "source": [
    "# First Convolutional Neural Network for Image Classification\n",
    "\n",
    "In this exercise, you will learn to build your first simple Convolutional Neural Network and use it to classify images. \n",
    "\n",
    "You will learn: \n",
    "- how to construct a Convolutional Neural Networks \n",
    "- adjust the different hyper-parameters of the network (e.g. number of filters, number of layers, etc) and observe the effects \n",
    "- how to visualize the activations of the hidden layers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Dataset\n",
    "\n",
    "We will be using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. \n",
    "\n",
    "![fashion-mnist](images/fashion-mnist.png)\n",
    "\n",
    "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n",
    "\n",
    "|Label|Class|\n",
    "|---|---|\n",
    "|0|T-shirt/top|\n",
    "|1|Trouser|\n",
    "|2|Pullover|\n",
    "|3|Dress|\n",
    "|4|Coat|\n",
    "|5|Sandal|\n",
    "|6|Shirt|\n",
    "|7|Sneaker|\n",
    "|8|Bag|\n",
    "|9|Ankle boot|       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data using `keras.datasets` as it is part of datasets available from keras.\n",
    "For a list of dataset available from keras, see https://www.tensorflow.org/api_docs/python/tf/keras/datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 605
    },
    "colab_type": "code",
    "id": "C0tFgT1MMKi6",
    "outputId": "b9c48f3c-639a-4c14-ebbe-657cacca81f8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print('Shape of training_images = {}'.format(training_images.shape))\n",
    "print('Shape of test_images = {}'.format(test_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the images\n",
    "\n",
    "\n",
    "\n",
    "You need to preprocess the image before using it as the input to the CNN.\n",
    "CNN expects our input to be of the shape (batch, heigt, width, channels). Furthermore, the pixel values of the original image is in the range (0,255). Neural network will learn better if the input values are normalized to between (0.0, 1.0). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to a 4-D tensors, with number of channel as 1, since this is a gray scale image\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "\n",
    "# scale the input to between 0. and 1.0\n",
    "training_images=training_images / 255.0\n",
    "test_images=test_images/255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your first CNN\n",
    "\n",
    "A typical CNN consists of 1 or more blocks of Conv2D layer followed by MaxPooling2D layer. The 2D array from the last convolutional block will then be flattened into 1D array before feeding into Dense (fully connected) layer for classification. The last layer uses `softmax` to ouput the probabilities of each of the 10-classes. Note that the last layer has to have same number of output units as the number of classes (in our case, we have 10 classes, so we need 10 output units). Look at the model summary carefully and make sure you understand why the output shape is as shown and also how to calculate the number of parameters. \n",
    "\n",
    "\n",
    "**Exercise**:\n",
    "\n",
    "Construct a convnet that consist of following: \n",
    "- Conv layer with 32 filters of size 3x3, and using 'relu' activation function, followed by Max Pooling layer of pool size 2x2. \n",
    "- Conv layer with 64 filters of size 3x3, and using 'relu' activation function, followed by Max Pooling layer of pool size 2x2. \n",
    "- Flatten the 2D array into 1D\n",
    "- Fully connected layer with 128 neurons, using 'relu' activation function\n",
    "- Fully connected layer with 10 neurons with a softmax function. \n",
    "\n",
    "Use Adam optimizer and specify 'sparse_categorical_crossentropy' as loss function. \n",
    "\n",
    "***Note***: If you are using one-hot-encoding for your output label, then you should specify 'categorical_crossentropy' as your loss function.\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "    \n",
    "```\n",
    "model = Sequential([\n",
    "  Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(2, 2),\n",
    "  Conv2D(64, (3,3), activation='relu'),\n",
    "  MaxPooling2D(2,2),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "### Start your code here ###\n",
    "\n",
    "\n",
    "### End your code ###\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also plot the model to a png file\n",
    "\n",
    "#plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training_images, training_labels, batch_size=256, epochs=5, validation_data=(training_images, training_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the training vs validation accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model is doing vey well on both training and validation accuracy, achieving close to 99% accuracy. However, as you can see from the plots, the accuracy goes up and down as training epochs goes, it is a bit difficult to see the trend. We can smooth out the graph using smoothing average:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXx_LX3SAlFs"
   },
   "source": [
    "## Visualizing the Convolutions and Pooling\n",
    "\n",
    "It is often said that deep learning network is a blackbox. However, this is certainly not true for Convnets. The representations learnt by Convnets are highly interpretable, as they are representations of visual concepts. \n",
    "\n",
    "The following codes allows us to visualize the output of the feature maps learnt by Convnet. By looking at output (activations) of these feature maps, for different kind of images, we will understand how a specific image is being classified. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first print out the labels of the first 10 test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look two different images, image 0 with label 9 (ankle boot) and image 2 with label 1 (trouser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "f-6nX4QsOku6",
    "outputId": "6b85ed93-6868-4c2c-b066-0808d6536878"
   },
   "outputs": [],
   "source": [
    "ANKLE_BOOT_IDX = 0\n",
    "TROUSER_IDX = 2\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.imshow(test_images[ANKLE_BOOT_IDX].reshape(28,28))\n",
    "ax2.imshow(test_images[TROUSER_IDX].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create activation model for each individual layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# extract the outputs of first four layers (only the Conv2D, MaxPooling2D layers)\n",
    "layer_outputs = [layer.output for layer in model.layers][:4]\n",
    "\n",
    "# create activation models that will return these outputs given the model input\n",
    "activation_model_conv1 = Model(inputs=model.input, outputs=layer_outputs[0])\n",
    "activation_model_pool1 = Model(inputs=model.input, outputs=layer_outputs[1])\n",
    "activation_model_conv2 = Model(inputs=model.input, outputs=layer_outputs[2])\n",
    "activation_model_pool2 = Model(inputs=model.input, outputs=layer_outputs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at activations from the 1st Conv2D layer for both images. There are 64 filter maps from the 2nd Conv layer, but we going to look at only the first 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(2,10, figsize=(20,4))\n",
    "ankle_boot_activations_conv1 = activation_model_conv1.predict(test_images[ANKLE_BOOT_IDX].reshape(1, 28, 28, 1))\n",
    "trouser_activations_conv1 = activation_model_conv1.predict(test_images[TROUSER_IDX].reshape(1, 28, 28, 1))\n",
    "\n",
    "for filter_idx in range(0, 10):\n",
    "    axarr[0, filter_idx].imshow(ankle_boot_activations_conv1[0,:,:, filter_idx])\n",
    "    axarr[1, filter_idx].imshow(trouser_activations_conv1[0,:,:,filter_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we can see that 1st Conv layer seems to act as detector of lines and edges. Some filter such as filter 0 is more like an vertical line detector, whereas some filter such as filter 4 seems to detect edges of the shape.\n",
    "\n",
    "Your filter output may not be the same as we have shown here as the specific filters learnt by the Conv layer are not deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Now, complete the code below to examine the activations from the 2nd Conv2D layer for both images. There are 64 filter maps from the 2nd Conv layer, but we going to look at only the first 10.  What do you observe?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "<br/>\n",
    "   \n",
    "```\n",
    "fig, axarr = plt.subplots(2,10, figsize=(20,4))\n",
    "\n",
    "ankle_boot_activations_conv2 = activation_model_conv2.predict(test_images[ANKLE_BOOT_IDX].reshape(1, 28, 28, 1))\n",
    "trouser_activations_conv2 = activation_model_conv2.predict(test_images[TROUSER_IDX].reshape(1, 28, 28, 1))\n",
    "\n",
    "for filter_idx in range(0, 10):\n",
    "    axarr[0, filter_idx].imshow(ankle_boot_activations_conv2[0,:,:, filter_idx])\n",
    "    axarr[1, filter_idx].imshow(trouser_activations_conv2[0,:,:,filter_idx])\n",
    "    \n",
    "```\n",
    "</details>\n",
    "\n",
    "You will observe that the outputs seems to be more abstract and seems to detect a higher-level construct, such a the presence of certain part of the object (e.g. the collar part of the boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start your code here ###\n",
    "\n",
    "\n",
    "### End your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, complete the code below to examine the activations from the last maxpooling layer for both images. There are 64 filter maps from the 2nd Conv layer, but we going to look at only the first 10.  What do you observe?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "<br/>\n",
    "   \n",
    "```\n",
    "fig, axarr = plt.subplots(2,10, figsize=(20,4))\n",
    "\n",
    "ankle_boot_activations_pool2 = activation_model_pool2.predict(test_images[ANKLE_BOOT_IDX].reshape(1, 28, 28, 1))\n",
    "trouser_activations_pool2 = activation_model_pool2.predict(test_images[TROUSER_IDX].reshape(1, 28, 28, 1))\n",
    "\n",
    "for filter_idx in range(0, 10):\n",
    "    axarr[0, filter_idx].imshow(ankle_boot_activations_pool2[0,:,:, filter_idx])\n",
    "    axarr[1, filter_idx].imshow(trouser_activations_pool2[0,:,:,filter_idx])\n",
    "\n",
    "    \n",
    "```\n",
    "</details>\n",
    "\n",
    "The MaxPooling2D just highlight or emphasize more sharply the abstract part detected by the Conv layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start your code here ###\n",
    "\n",
    "\n",
    "### End your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MaxPooling2D just highlight or emphasize more sharply the abstract part detected by the Conv layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KVPZqgHo5Ux"
   },
   "source": [
    "**Additional Exercises:**\n",
    "\n",
    "1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\n",
    "\n",
    "2. Remove the final Convolution. What impact will this have on accuracy or training time?\n",
    "\n",
    "3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.\n",
    "\n",
    "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "First CNN for Image Classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
