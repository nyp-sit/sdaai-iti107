{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-pdc2-students/blob/master/iti107/session-3/baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "Welcome to this week's programming exercise. In this exercise, we will be training a model to recognise if an image depicts positive (e.g. happy, pleasant, beautiful) or negative (e.g. sad, angry, death, etc) emotion . We will first train a baseline model without using transfer learning. The dataset is a collection of around 1600 images from Flickr, and labelled with Positive or Negative label. We only apply data augmentation to our training set. In the next exercise, we will use transfer learning technique to train another model and compare the performance of both.\n",
    "\n",
    "At the end of this exercise, you will be able to: \n",
    "- use ImageDataGenerator to generate augmented images from directory \n",
    "- understand typical directory structure expected by the ImageDataGenerator\n",
    "- train a convolutional neural network using the ImageDataGenerator\n",
    "- visualize the training/validation loss/accuracy over training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "#from utils import *\n",
    "from utils import prepare_data, download_trained_model_and_history\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "To avoid cluttering the codes in the notebook, we put the `prepare_data()` code in a separate python file. This function prepare the directory structure (by creating a **train** and **valid** subfolders under `data_path` directory for holding the train and validation data respectively). It also automatically unzip and copy the image files into 'Negative' and 'Positive' subfolders of the training and validation folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "models_path = \"models\"\n",
    "if not os.path.exists(models_path):\n",
    "    os.mkdir(models_path)\n",
    "valid_size = 0.2    # validation split \n",
    "FORCED_DATA_REWRITE = True  # remove old data if they exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, valid_path = prepare_data(data_path=data_path, \n",
    "                                      valid_size=valid_size, \n",
    "                                      FORCED_DATA_REWRITE=FORCED_DATA_REWRITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_path = os.path.join(train_path, \"Negative\")\n",
    "train_pos_path = os.path.join(train_path, \"Positive\")\n",
    "valid_neg_path = os.path.join(valid_path, \"Negative\")\n",
    "valid_pos_path = os.path.join(valid_path, \"Positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly select `n_examples` and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 5\n",
    "np.random.seed(42)\n",
    "positive_expamples = np.random.choice(os.listdir(train_pos_path), size=n_examples, replace=False)\n",
    "negative_expamples = np.random.choice(os.listdir(train_neg_path), size=n_examples, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, n_examples * 2))\n",
    "for i in range(n_examples):\n",
    "    plt.subplot(n_examples, 2, i * 2 + 1)\n",
    "    img = load_img(os.path.join(train_pos_path, positive_expamples[i]))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    if i == 0:\n",
    "        plt.title(\"Positive\", fontsize=18)\n",
    "    plt.subplot(n_examples, 2, i * 2 + 2)\n",
    "    img = load_img(os.path.join(train_neg_path, negative_expamples[i]))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    if i == 0:\n",
    "        plt.title(\"Negative\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Data Generator\n",
    "\n",
    "We will use the tf.keras ImageDataGenerator to serve the training and validation data from the directory. For the training data, we will apply some data augmentation techniques such as rotaion, shifting, shearing, etc. We will also need to normalize the image pixel values to between 0.0 and 1.0. \n",
    "\n",
    "***Note about Python generator***\n",
    "\n",
    "A Python generator is an object that acts as an iterator: it’s an object you can use\n",
    "with the `for … in` loop. Generators are built using the yield operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=40, \n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Create an ImageDataGenerator for validation data too. Do you need to apply data transformation for validation data?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "We only apply transformation for training data, and not validation data.\n",
    "\n",
    "valid_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "\n",
    "valid_datagen = None\n",
    "\n",
    "### END YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `flow_from_directory()` method to generate batches of data from the specified train and validation directory. The directory should contain one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator.  Since we only have 2 classes, we specify `class_mode` as 'binary' so that the generator will return the binary labels (0 and 1). The class name mapping for the labels will be based on the names of the subdirectories (in our case 'Negative' and 'Positive'). The `batch_size` determines how many samples are returned by the generator on each iteration.\n",
    "\n",
    "See [Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) for more details of the different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img_height, img_width = 300, 400\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(train_path, \n",
    "                                              target_size=(img_height, img_width), \n",
    "                                              class_mode=\"binary\", \n",
    "                                              batch_size=64, \n",
    "                                              shuffle=True, \n",
    "                                              seed=21)\n",
    "\n",
    "valid_gen = valid_datagen.flow_from_directory(valid_path, \n",
    "                                              target_size=(img_height, img_width), \n",
    "                                              class_mode=\"binary\", \n",
    "                                              batch_size=64, \n",
    "                                              shuffle=False, \n",
    "                                              seed=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the class names to class labels mapping\n",
    "train_gen.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this on non-GPU, the training could take quite a while. To save time, you can set LOAD_BASELINE_MODEL = True and it will download the model we have previously trained as well as the training history to the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_BASELINE_MODEL = False\n",
    "\n",
    "if LOAD_BASELINE_MODEL: \n",
    "    #download_trained_model_and_history(os.path.join(models_path, 'baseline.model'))\n",
    "    download_trained_model_and_history(os.path.join(models_path, \"baseline.model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if LOAD_BASELINE_MODEL:\n",
    "    try:\n",
    "        model_baseline = load_model(os.path.join(models_path, \"baseline.model.h5\"))\n",
    "        print(\"Model has been loaded!\")\n",
    "    except:\n",
    "        LOAD_BASELINE_MODEL = False\n",
    "        print(\"Load has failed. Model will be built from scratch.\")\n",
    "        \n",
    "if not LOAD_BASELINE_MODEL:\n",
    "    \n",
    "    inp = Input(shape=train_gen.target_size + (3,))\n",
    "\n",
    "    conv1 = Conv2D(filters=64, kernel_size=(3, 3), strides=(2, 2))(inp)\n",
    "    conv2 = Conv2D(filters=64, kernel_size=(3, 3), strides=(2, 2))(conv1)\n",
    "    maxpool1 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(filters=128, kernel_size=(3, 3), strides=(2, 2))(maxpool1)\n",
    "    conv4 = Conv2D(filters=128, kernel_size=(3, 3), strides=(2, 2))(conv3)\n",
    "    maxpool2 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    flattened = Flatten()(maxpool2)\n",
    "    \n",
    "    fc1 = Dense(units=256, activation=\"relu\", \n",
    "                kernel_initializer=\"he_normal\")(flattened)\n",
    "    dp1 = Dropout(rate=0.5)(fc1)\n",
    "    \n",
    "    fc2 = Dense(units=512, activation=\"relu\", \n",
    "                kernel_initializer=\"he_normal\")(dp1)\n",
    "    dp2 = Dropout(rate=0.5)(fc2)\n",
    "    \n",
    "    out = Dense(units=1, activation=\"sigmoid\")(dp2)\n",
    "    \n",
    "    model_baseline = Model(inputs=[inp], outputs=[out])\n",
    "    \n",
    "    model_baseline.compile(optimizer=\"Adam\", \n",
    "                           loss=\"binary_crossentropy\", \n",
    "                           metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(\"Model has been built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_baseline.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Because data is drawn endlessly from generator, you need to tell Keras model how many samples to draw from generator before declaring an epoch is over. This is the the role of `steps_per_epoch`. \n",
    "\n",
    "Below, we set the `steps_per_epoch` to be equal to 'number of samples/batch size'. However, this is kind of arbitrary, and it does not mean the generator will return all the images available in the directory. For example, if we have 100 different images in the directory and our batch size is 10, our steps_per_epoch = 100/10, i.e. 10. However, after 10 steps of 10 images, for a total of 100 generated images, not all the original 100 images in the directory will be used. This is because ImageDataGenerator randomly transforms the images, and you may get two slightly transformed versions of the same image, instead of 2 different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = int(np.ceil(train_gen.n * 1. / train_gen.batch_size))\n",
    "valid_steps_per_epoch = int(np.ceil(valid_gen.n * 1. / valid_gen.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not LOAD_BASELINE_MODEL:\n",
    "    hist_baseline = model_baseline.fit(train_gen, \n",
    "                                                 steps_per_epoch=train_steps_per_epoch, \n",
    "                                                 epochs=15, \n",
    "                                                 validation_data=valid_gen, \n",
    "                                                 validation_steps=valid_steps_per_epoch)\n",
    "    # save the trained model\n",
    "    # we save the model in h5 format instead of the default SavedModel format due to an issue as highlighted here:\n",
    "    # https://github.com/tensorflow/tensorflow/issues/33454\n",
    "    model_baseline.save(os.path.join(models_path, \"baseline.model.h5\"))\n",
    "    \n",
    "    # save the history of training\n",
    "    with open('baseline.history', 'wb') as f:\n",
    "        pickle.dump(hist_baseline.history, f)\n",
    "    hist_baseline = hist_baseline.history\n",
    "else:\n",
    "    with open('baseline.history', 'rb') as f:\n",
    "        hist_baseline = pickle.load(f)\n",
    "    print(\"Model has already been trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.suptitle(\"Training evolution for homegrown model\", fontsize=18)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(hist_baseline[\"loss\"], label=\"Train\")\n",
    "plt.plot(hist_baseline[\"val_loss\"], label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Crossentropy loss\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.array(hist_baseline[\"accuracy\"]) * 100, label=\"Train\")\n",
    "plt.plot(np.array(hist_baseline[\"val_accuracy\"]) * 100, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Accuracy, %\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the plot, the validation accuracy fluctuates around 50% point. Our model is no better than random guess !! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report on Test Data \n",
    "\n",
    "By right, you should have allocated some data as test set for your test model. Since our data is pretty small, we did not. But for the sake of having better idea how our model is faring on each class, let's just use our validation data for getting some hard numbers :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_baseline.predict(valid_gen, valid_steps_per_epoch)\n",
    "y_valid = np.array(valid_gen.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, y_pred.flatten() > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our model almost always predict 1 (Positive) emotion!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tf2env",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
