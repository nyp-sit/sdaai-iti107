{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.comgithub/nyp-sit/sdaai-iti107/blob/main/session-3/improved_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved model using Transfer Learning\n",
    "\n",
    "Welcome to this week's programming exercise. In this exercise, we use transfer learning to improve our baseline model. We make use of a model (VGG19) that is already trained on ImageNet and use the convolutional neural network as a feature extractor and train a classifier specifically for our emotion classification task.\n",
    "\n",
    "At the end of this exercise, you will be able to: \n",
    "- understand how to load a pretrained model with and without the classification layer  \n",
    "- extract training features using the pre-trained model as feature extractor\n",
    "- train a classifier using the extracted features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from utils import prepare_data\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, \\\n",
    "                            precision_recall_curve, average_precision_score, confusion_matrix\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fix_cudnn_bug\n",
    "fix_cudnn_bug()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "models_path = \"models\"\n",
    "validation_size = 0.2\n",
    "FORCED_DATA_REWRITE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, valid_path = prepare_data(data_path=data_path, \n",
    "                                      valid_size=validation_size, \n",
    "                                      FORCED_DATA_REWRITE=FORCED_DATA_REWRITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_path = os.path.join(train_path, \"Negative\")\n",
    "train_pos_path = os.path.join(train_path, \"Positive\")\n",
    "valid_neg_path = os.path.join(valid_path, \"Negative\")\n",
    "valid_pos_path = os.path.join(valid_path, \"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this to larger or smaller size\n",
    "img_height, img_width = 150, 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model as Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using VGG19 as our pretrained model (you can choose any other pretrained model, such as ResNet, etc). Keras comes with a set of [pretrained models](https://www.tensorflow.org/api_docs/python/tf/keras/applications) you can choose from. In the following call, we load the model VGG19 without including the classification layers (`include_top=False`). In the weights, we specify that we want to download the weights that was trained on ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = VGG19(include_top=False, \n",
    "                         weights=\"imagenet\",  \n",
    "                         input_shape=(img_height, img_width, 3))\n",
    "\n",
    "model_pretrained.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "- What is the last layer in the pretrained model and what is the output shape? Do you have any Fully connected layers?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "The last layer is the MaxPooling2D layer. The output is a 512 feature maps of 9x12 size. There is no Fully connected (Dense) layers. The network is a convolutional base network.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = train_datagen.flow_from_directory(train_path, \n",
    "                                        target_size=(img_height, img_width), \n",
    "                                        class_mode=None, \n",
    "                                        batch_size=8, \n",
    "                                        shuffle=False)\n",
    "\n",
    "valid_gen = valid_datagen.flow_from_directory(valid_path, \n",
    "                                        target_size=(img_height, img_width), \n",
    "                                        class_mode=None, \n",
    "                                        batch_size=8, \n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = int(np.ceil(train_gen.n * 1. / train_gen.batch_size))\n",
    "print('num of train steps per epoch {}'.format(train_steps_per_epoch))\n",
    "valid_steps_per_epoch = int(np.ceil(valid_gen.n * 1. / valid_gen.batch_size))\n",
    "print('num of validation steps per epoch {}'.format(valid_steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features on the train set \n",
    "\n",
    "We use `predict()` to loop through all the train images (and also the validation images). The output will be the features spit out by the convolutional base. We will then use these features as our training samples instead of the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = model_pretrained.predict(train_gen, \n",
    "                                    steps=train_steps_per_epoch, \n",
    "                                    verbose=1)\n",
    "\n",
    "y_train = train_gen.classes\n",
    "\n",
    "np.save(os.path.join(train_path, \"train_features.npy\"), X_train)\n",
    "np.save(os.path.join(train_path, \"train_classes.npy\"), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = model_pretrained.predict(valid_gen, \n",
    "                                     steps=valid_steps_per_epoch, \n",
    "                                     verbose=1)\n",
    "\n",
    "y_valid = valid_gen.classes\n",
    "\n",
    "np.save(os.path.join(valid_path, \"valid_features.npy\"), X_valid)\n",
    "np.save(os.path.join(valid_path, \"valid_classes.npy\"), y_valid)\n",
    "\n",
    "print(\"Features are calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model\n",
    "\n",
    "Now we will build a new model that takes in the extracted features as input. Instead of the usual flatten layer, followed by dense layers, let us use a GAP layer, followed by Dense, a Droput and another Dense that output the prediction. \n",
    "\n",
    "**Questions:**\n",
    "\n",
    "1. What should be input shape to our model? \n",
    "2. What is the output shape of the Global Average Pooling (GAP) layer? \n",
    "3. How many units we need for output, and what should we use as activation function? \n",
    "\n",
    "Complete the code below. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "1. The input shape should be (9, 12, 512) which is the output shape of our convolutional base\n",
    "2. The output shape of GAP is (512) since the maxpooling layer (the last layer) of the convolutional base has 512 feature maps (channels). \n",
    "3. We need only 1 output unit as we are doing binary classification (0 or 1) and we should use 'sigmoid' as the activation function for binary classification. \n",
    "\n",
    "Codes: \n",
    "\n",
    "```\n",
    "inp = Input(shape=X_train.shape[1:])\n",
    "fl = GlobalAveragePooling2D()(inp)\n",
    "fc1 = Dense(units=512, activation=\"relu\", kernel_initializer=\"he_normal\")(fl)\n",
    "dp1 = Dropout(rate=0.5)(fc1)\n",
    "out = Dense(units=1, activation=\"sigmoid\")(dp1)\n",
    "\n",
    "model_top = Model(inputs=[inp], outputs=[out], name=\"top\")\n",
    "model_top.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=Adam(lr=0.0001), \n",
    "                  metrics=[\"accuracy\"])\n",
    "``` \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model here, you can use either Keras Sequential or functional API to build your model\n",
    "### START YOUR CODE HERE ###\n",
    "\n",
    "model_top = None\n",
    "\n",
    "### END YOUR CODE HERE ###    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train our classifier we the extracted features (X_train) for 100 epochs. The training will be fast, as we only have very few parameters (around 200k) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PRETRAINED_MODEL:\n",
    "    %time hist_top = model_top.fit(X_train, y_train, \\\n",
    "                                   epochs=100, \\\n",
    "                                   validation_data=(X_valid, y_valid), \\\n",
    "                                   verbose=1)\n",
    "    model_top.save(os.path.join(models_path, \"model_top.h5\"))\n",
    "    # save the history of training\n",
    "    with open(os.path.join(models_path, 'hist_top.history'), 'wb') as f:\n",
    "        pickle.dump(hist_top.history, f)\n",
    "    hist_top = hist_top.history\n",
    "else:\n",
    "    with open(os.path.join(models_path, 'hist_top.history'), 'rb') as f:\n",
    "        hist_top = pickle.load(f)\n",
    "    \n",
    "    print(\"Model has already been trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.suptitle(\"Training progress for pretrained model\", fontsize=20)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(hist_top[\"loss\"], label=\"Train\")\n",
    "plt.plot(hist_top[\"val_loss\"], label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Crossentropy loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.array(hist_top[\"accuracy\"]) * 100, label=\"Train\")\n",
    "plt.plot(np.array(hist_top[\"val_accuracy\"]) * 100, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Accuracy, %\")\n",
    "plt.xlabel(\"Epoch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_top.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, y_pred.flatten() > 0.5))\n",
    "print(\"Accuracy = {:.1f}%\".format(accuracy_score(y_valid, y_pred.flatten() > 0.5) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an good improvement in the model (should be around 20%). The model also takes much less time to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model for deployment\n",
    "\n",
    "We cannot just use our `model_top` that is trained for image classification, as it take extracted features as input, not image. We need to stick back our convolutional base and use an input layer of appropriate shape. This is what we are going to do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(img_height, img_width, 3))\n",
    "pretrained_output = model_pretrained(inp)\n",
    "top_output = model_top(pretrained_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = Model(inputs=[inp], outputs=[top_output])\n",
    "model_final.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.save(os.path.join(models_path, \"pretrained_full.model.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let just test our full model on the images from validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model_final.predict(valid_gen, valid_steps_per_epoch)\n",
    "\n",
    "# convert probabilities into classification label based on threshold of 0.5 \n",
    "y_pred = y_pred_probs > 0.5\n",
    "y_valid = np.array(valid_gen.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra exercises\n",
    "\n",
    "1. Notice that we did not use data augmentation in the codes above.  You can try to add data augmentation and see if you can further improve the result.\n",
    "\n",
    "2. Train with the convolutional base, but unfreeze the last few convolutional layer, and see if it improve the model (you can refer to ipython notebook *fine_tuning.ipynb* on how to do it."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tf2env",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
