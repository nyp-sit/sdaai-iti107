{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the convolutional layers\n",
    "\n",
    "The code below shows how we can unfreeze last few layers to allow fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, \\\n",
    "                            precision_recall_curve, average_precision_score, confusion_matrix\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/datasets/intel_emotions_dataset.zip\n",
      "504274944/504272804 [==============================] - 54s 0us/step\n",
      ".\\datasets\\intel.zip\n"
     ]
    }
   ],
   "source": [
    "dataset_URL = 'https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/datasets/intel_emotions_dataset.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('intel.zip', origin=dataset_URL, extract=True, cache_dir='.')\n",
    "print(path_to_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\datasets\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.dirname(path_to_zip)\n",
    "print(dataset_dir)\n",
    "pos_path = os.path.join(dataset_dir, 'Positive')\n",
    "neg_path = os.path.join(dataset_dir, 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 150, 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1305 images belonging to 2 classes.\n",
      "Found 325 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = datagen.flow_from_directory(train_path, \n",
    "                                        target_size=(img_height, img_width), \n",
    "                                        class_mode='binary', \n",
    "                                        batch_size=16, \n",
    "                                        shuffle=False)\n",
    "\n",
    "valid_gen = datagen.flow_from_directory(valid_path, \n",
    "                                        target_size=(img_height, img_width), \n",
    "                                        class_mode='binary', \n",
    "                                        batch_size=16, \n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model as Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base = VGG19(include_top=False, \n",
    "                         weights=\"imagenet\",  \n",
    "                         input_shape=(img_height, img_width, 3))\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 4, 4, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 20,287,553\n",
      "Trainable params: 20,287,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(units=512, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set the conv_base (all layers to Trainable) and free all the earlier layers except 20 onwards. \n",
    "Note: Previously we could set trainable to false, and then only unfreeze a specific layer, but seems that it is not working anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:20]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 4, 4, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 20,287,553\n",
      "Trainable params: 2,622,977\n",
      "Non-trainable params: 17,664,576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we print another summary and we can see that the trainable weights are now smaller (2,622,977 vs 20,287,553 previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 4, 4, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 20,287,553\n",
      "Trainable params: 2,622,977\n",
      "Non-trainable params: 17,664,576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=Adam(lr=0.0001), \n",
    "                  metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = int(np.ceil(train_gen.n * 1. / train_gen.batch_size))\n",
    "valid_steps_per_epoch = int(np.ceil(valid_gen.n * 1. / valid_gen.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "82/82 - 6s - loss: 0.8322 - accuracy: 0.5073 - val_loss: 0.6508 - val_accuracy: 0.6400\n",
      "Epoch 2/20\n",
      "82/82 - 5s - loss: 0.6891 - accuracy: 0.5709 - val_loss: 0.6990 - val_accuracy: 0.5569\n",
      "Epoch 3/20\n",
      "82/82 - 5s - loss: 0.6658 - accuracy: 0.6123 - val_loss: 0.7173 - val_accuracy: 0.5631\n",
      "Epoch 4/20\n",
      "82/82 - 5s - loss: 0.6595 - accuracy: 0.6314 - val_loss: 0.6431 - val_accuracy: 0.6246\n",
      "Epoch 5/20\n",
      "82/82 - 5s - loss: 0.6633 - accuracy: 0.6077 - val_loss: 0.6674 - val_accuracy: 0.5815\n",
      "Epoch 6/20\n",
      "82/82 - 5s - loss: 0.5751 - accuracy: 0.7004 - val_loss: 0.6422 - val_accuracy: 0.6277\n",
      "Epoch 7/20\n",
      "82/82 - 5s - loss: 0.5759 - accuracy: 0.6866 - val_loss: 0.6564 - val_accuracy: 0.6031\n",
      "Epoch 8/20\n",
      "82/82 - 5s - loss: 0.5547 - accuracy: 0.7188 - val_loss: 0.5942 - val_accuracy: 0.7015\n",
      "Epoch 9/20\n",
      "82/82 - 5s - loss: 0.5196 - accuracy: 0.7502 - val_loss: 0.6034 - val_accuracy: 0.7108\n",
      "Epoch 10/20\n",
      "82/82 - 5s - loss: 0.4814 - accuracy: 0.7824 - val_loss: 0.6731 - val_accuracy: 0.6062\n",
      "Epoch 11/20\n",
      "82/82 - 5s - loss: 0.4246 - accuracy: 0.8069 - val_loss: 0.7542 - val_accuracy: 0.5785\n",
      "Epoch 12/20\n",
      "82/82 - 5s - loss: 0.4044 - accuracy: 0.8192 - val_loss: 0.6190 - val_accuracy: 0.6892\n",
      "Epoch 13/20\n",
      "82/82 - 5s - loss: 0.3710 - accuracy: 0.8375 - val_loss: 0.5995 - val_accuracy: 0.7169\n",
      "Epoch 14/20\n",
      "82/82 - 5s - loss: 0.3122 - accuracy: 0.8851 - val_loss: 0.6097 - val_accuracy: 0.7108\n",
      "Epoch 15/20\n",
      "82/82 - 5s - loss: 0.2618 - accuracy: 0.9080 - val_loss: 0.6825 - val_accuracy: 0.6646\n",
      "Epoch 16/20\n",
      "82/82 - 5s - loss: 0.2058 - accuracy: 0.9341 - val_loss: 0.7360 - val_accuracy: 0.6615\n",
      "Epoch 17/20\n",
      "82/82 - 5s - loss: 0.2182 - accuracy: 0.9103 - val_loss: 0.6970 - val_accuracy: 0.6708\n",
      "Epoch 18/20\n",
      "82/82 - 5s - loss: 0.1574 - accuracy: 0.9632 - val_loss: 0.7512 - val_accuracy: 0.6523\n",
      "Epoch 19/20\n",
      "82/82 - 5s - loss: 0.1349 - accuracy: 0.9648 - val_loss: 0.7346 - val_accuracy: 0.7015\n",
      "Epoch 20/20\n",
      "82/82 - 5s - loss: 0.0864 - accuracy: 0.9854 - val_loss: 0.7885 - val_accuracy: 0.7200\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_gen,\n",
    "      steps_per_epoch=train_steps_per_epoch,\n",
    "      epochs=20,\n",
    "      validation_data=valid_gen, \n",
    "      validation_steps=valid_steps_per_epoch,\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy seems to be worse than before (i.e. without fine-tuning the convolutional layer and just trained the classification dense layer). One reason maybe that our training samples are too little. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
