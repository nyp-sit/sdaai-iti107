{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/main/session-6/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text using RNN\n",
    "\n",
    "Sequence model (such as RNN) is effective in learning a language model. In this programming exercise, we are going to make our RNN model read a lot of poems of William Shakespeare and use it to write poems in the style of Shakespeare!!\n",
    "\n",
    "![shakespeare](nb_images/shakespeare.jpg)\n",
    "\n",
    "\n",
    "**You will learn how to:**\n",
    "- set up a 'vanila' RNN model \n",
    "- prepare the input data for learning a character-level language model\n",
    "- generate and sample character from the RNN output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "## Prepare Data\n",
    "\n",
    "For this example, we will use the Shakespeare's Sonnets as our training corpus. Change the following line to run this code on your own text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare_sonnets.txt','https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/shakespeare_sonnets.txt')\n",
    "# Read, we assume the input text is utf-8. ignore non-utf-8 char if it happend to be in the text\n",
    "text = open(path_to_file, 'rb').read().decode('utf-8', 'ignore')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhg9NrUymwO"
   },
   "outputs": [],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Character-based language model**\n",
    "\n",
    "For language model, we can either choose to use word or character as our vocabulary for our model. In this exercise, we will be buidling a character-level language model. The benefit of character-based language models is their small vocabulary and flexibility in handling any words, punctuation, and other document structure.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Create a vocabulary based on all unique characters in the text corpus. (Hint: use a sorted set). \n",
    "\n",
    "What is your vocabulary size? \n",
    "\n",
    "<br/>\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```\n",
    "vocab = sorted(set(text))\n",
    "```\n",
    "\n",
    "there are total of 65 unique characters\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlCgQBRVymwR"
   },
   "outputs": [],
   "source": [
    "# Get the unique characters in the file\n",
    "\n",
    "### START YOUR CODE HERE (~1 line) ###\n",
    "\n",
    "\n",
    "## END YOUR CODE ###\n",
    "\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vectorize the text\n",
    "\n",
    "As neural network only deals with the numbers, we need to map our text to a numerical representation. We will create two mapping tables: one mapping characters to numbers (integer numbers), and another for numbers to characters. The reverse mapping (numbers to characters) will be used to convert the model output (which are in numbers) to recognizable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "\n",
    "#enumerate() in python will iterate over the list and return the index (0, 1, 2, ..) in addition to the element itself.\n",
    "#in the line below, we swap the place of i, and u so that the dictionary key is u (i.e. char) and the value is i (i.e. index)\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# mapping of index to character\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`char2idx` mapping table can now be used to convert the text to numbers (integers).\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Convert `text` into np.array of integers. \n",
    "\n",
    "***Hint*** Use list comprehension for text by mapping each character to numbers using `char2idx` and then convert the list of numbers to numpy array by call np.array(list). \n",
    "\n",
    "<br/>\n",
    "\n",
    "<details><summary>Click here for answer</summary>  \n",
    "    \n",
    "```\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1VKcQHcymwb"
   },
   "outputs": [],
   "source": [
    "# Show how the first 30 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:30]), text_as_int[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### Formulate the learning task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "This is what we want to achieve for our model: given a character, or a sequence of characters, what is the most probable next character? \n",
    "\n",
    "So how should we create our training samples? The input to the model will be a sequence of characters, and the target sequence will be the same sequence of characters, but offset by one timestep.  For example, given the the corpus text such as \n",
    "```\n",
    "From fairest creatures we desire increase, \n",
    "That thereby beauty's rose might never die,\n",
    "```\n",
    "\n",
    "If we fix our timestep as 10 (input sequence length is 10), we can choose our train input sequence to be 'From faire', and our target sequence be 'rom faires'. So basically we are training our model to predict next character correctly by minimizing cross entropy loss between expected output character and predicted output character across all time steps. \n",
    "\n",
    "We can visualize the training steps as follow: \n",
    "\n",
    "\n",
    "<img src=\"nb_images/training_samples.png\" style=\"width:400;height:300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Create training examples and targets\n",
    "\n",
    "We will divide the text into sample sequences. Each sample sequence will contain `seq_length` characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding target sequence contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "One way to create sample sequences are to divide the corpus into sequences of `seq_length+1` characters. The 1st sequence will take `seq_length+1` characters starting from `0`, 2nd sequence will take `seq_length+1` characters starting from `seq_length+1`, and 3rd sequence will take `seq_length+1` characters starting from `2*(seq_length+1)` and so on. For each sequence, `[0:seq_length]` becomes input sequence, and `[1:seq_length+1]`, i.e. offset by 1, becomes target sequence. \n",
    "\n",
    "\n",
    "Complete the codes below. \n",
    "\n",
    "<br/>\n",
    "\n",
    "<details><summary>Click here for answer</summary>  \n",
    "    \n",
    "```\n",
    " x = text[start_idx:end_idx][:seq_length]\n",
    " y = text[start_idx:end_idx][1:seq_length+1]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_samples(text, seq_length):\n",
    "    \"\"\"\n",
    "    Create samples of x (input) and y (target) character sequences of length seq_length from the given corpus text\n",
    "\n",
    "    Arguments:\n",
    "    text -- the corpus text (as integers)\n",
    "    seq_length -- the length of the character sequence (i.e. the number of time steps)\n",
    "\n",
    "    Returns:\n",
    "    X -- a list of input character (as integers) sequences \n",
    "    Y -- a list of target character (as integers) sequences \n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    # find out how many samples of (seq_length + 1) can be created from text\n",
    "    num_samples = len(text) // (seq_length+1)\n",
    "    \n",
    "    for i in range(num_samples): \n",
    "        # offset by seq_length+1 each time\n",
    "        start_idx = i*(seq_length+1)\n",
    "        end_idx = start_idx + (seq_length+1)\n",
    "        \n",
    "        ### BEGIN YOUR CODE HERE \n",
    "\n",
    "        \n",
    "\n",
    "        ### END YOUR CODE HERE  \n",
    "        \n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "     \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(text_in_int):\n",
    "    decoded = ''.join([idx2char[c] for c in text_in_int])\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiCopyGZymwi"
   },
   "source": [
    "Create the training samples and print the first examples input and target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "num_samples = len(text) // (seq_length+1)\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_training_samples(text_as_int, seq_length=seq_length)\n",
    "print(repr(decode(X[0])))\n",
    "print(repr(decode(Y[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_33OHL3b84i0"
   },
   "source": [
    "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"T\" and tries to predict the index for \"h\" as the next character. At the next timestep, it does the same thing but the `RNN` considers the previous step's context in addition to the current input character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Create training batches\n",
    "\n",
    "We will need to shuffle the training sequences before training to reduce variance in our model. Of course we can use our beloved scikit-learn to do the shuffling, but here we want to introduce you the 'tensorflow-way' of doing data pipelining and transformation, by using `tf.data.Dataset`. We first create a `tf.data.Dataset` from the X and Y sequences and create a pipeline to shuffle and batching the data, by calling `shuffle()` and `batch()` on the pipeline.\n",
    "\n",
    "For shuffling operation, we need to specify a buffer size. This specifies the size of the buffer which will be filled with the data samples for us to randomly sampled from. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a batch_size\n",
    "BATCH_SIZE = 64\n",
    "# buffer size for shuffling\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "dataset_shuffled = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test our data pipeline to see if our sequences are shuffled properly and the batching is working. We should expect for each take() operation, we will get BATCH_SIZE of x and y, already shuffled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_shuffled.take(1):\n",
    "    print('shape of x={} and y={}'.format(x.shape, y.shape))\n",
    "    print('1st x sample of the batch = {}'.format(repr(decode(x[0]))))\n",
    "    print('1st y sample of the batch = {}'.format(repr(decode(y[0]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
    "\n",
    "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
    "* `tf.keras.layers.SimpleRNN`: A plain vanilla RNN with size `units=rnn_units` (You can replace it with better model like LSTM/GRU which will be covered next lesson). We also need to set `return_sequences=True` and `stateful=True`.\n",
    "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs.\n",
    "\n",
    "***Note***: \n",
    "\n",
    "We need to set `return_sequences` to **True**  to return the hidden state output for each input time step. We need the output at each timestep so as to compare the expected char at each time step. \n",
    "\n",
    "We are setting `stateful` to **True** so that the last state for each sample at index `i` in a batch will be used as initial state for the sample of index `i` in the following batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Complete the code below to build the model specified above. \n",
    "\n",
    "<br/>\n",
    "\n",
    "<details><summary>Click here for answer</summary>  \n",
    "\n",
    "```\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                         batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.SimpleRNN(rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    ### END YOUR CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkA5upJIJ7W7"
   },
   "source": [
    "For each character the model looks up the embedding, runs the Simple RNN one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
    "\n",
    "\n",
    "<img src=\"nb_images/text_generation_training.png\" style=\"width:600;height:450px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## Try the model\n",
    "\n",
    "Now let us run the model to see that it behaves as expected.\n",
    "\n",
    "First check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_batch, target_batch in dataset_shuffled.take(1):\n",
    "    batch_predictions = model(input_batch)\n",
    "    print(batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example the sequence length of the input is `100` but the model can be run on inputs of any sequence length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "You can develop a better understanding of the RNN model by looking at the shape of the different weights in RNN layer. For example, in RNN layer, there are 3 weights `Wx`, `Wh` and `bias` returned if you call `get_weights()`. See if you can guess the shape of the weights correctly. Also compare your calculations against the `param #` show in the `model.summary()` above for the RNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx, Wh, b = model.layers[1].get_weights()\n",
    "\n",
    "# Uncomment below to check your answer\n",
    "\n",
    "print('Wx={}'.format(Wx.shape))\n",
    "print('Wh={}'.format(Wh.shape))\n",
    "print('b={}'.format(b.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Note: It is important to _sample_ from this distribution instead of always take _argmax_ of the distribution as it can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM1Vbxs_URw5"
   },
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfLtsP3mUhCG"
   },
   "source": [
    "Decode these to see the text predicted by this untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input: \\n\", repr(decode(input_batch[0])))\n",
    "print(\"Next Char Predictions: \\n\", repr(decode(sampled_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we see that an untrained model does not really generate any interesting looking text, but some gibberish characters. We will see if our model can do better after being trained on the Shakespeare text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "### Attach an optimizer, and a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UAjbjY03eiQ4"
   },
   "source": [
    "We use `tf.keras.losses.sparse_categorical_crossentropy` as our loss function as our label is not one-hot-encoded,\n",
    "\n",
    "Because our model returns logits, we need to set the `from_logits` flag to True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HrXTACTdzY-"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    print(labels.shape)\n",
    "    print(logits.shape)\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "batch_loss  = loss(y[:BATCH_SIZE], batch_predictions)\n",
    "print(\"Prediction shape: \", batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeOXriLcymww"
   },
   "source": [
    "Configure the training procedure using the `tf.keras.Model.compile` method. We'll use `tf.keras.optimizers.Adam` with default arguments and the loss function as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss=loss)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XBUUavgF56"
   },
   "source": [
    "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "To keep training time reasonable, use 60 epochs to train the model. In Colab, set the runtime to GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK-hmKjYVoll"
   },
   "outputs": [],
   "source": [
    "history = model.fit(dataset_shuffled, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "### Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyeYRiuVjodY"
   },
   "source": [
    "To keep this prediction step simple, let us just use a single sample at a time (i.e. batch size of 1).\n",
    "\n",
    "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
    "\n",
    "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk2WJ2-XjkGz"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71xa6jnYVrAN"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### The prediction loop\n",
    "\n",
    "The following code block generates the text:\n",
    "\n",
    "* It starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
    "\n",
    "* Get the prediction distribution of the next character using the start string and the RNN state.\n",
    "\n",
    "* Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
    "\n",
    "* The RNN state returned by the model is fed back into the model so that it now has more context, instead of only one word. After predicting the next word, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words.\n",
    "\n",
    "\n",
    "![To generate text the model's output is fed back to the input](nb_images/text_generation_sampling.png)\n",
    "\n",
    "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. As our training samples are pretty small, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "     # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    # add in the batch dimension at axis=0\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    \n",
    "    temperature = 0.8\n",
    "    model.reset_states()\n",
    "    sys.stdout.write(start_string)\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions,0)   # predictions shape = (input_seq_len, vocab_size)\n",
    "        \n",
    "        predictions = predictions / temperature\n",
    "        \n",
    "        # sample char(s) from output distribution  \n",
    "        sampled = tf.random.categorical(predictions, num_samples=1)  # sampled shape = (input_seq_len, 1)\n",
    "    \n",
    "        # take the last char of the sampled sequence [-1]. 0 to access the first element of second axis\n",
    "        predicted_id = sampled[-1,0].numpy()\n",
    "        \n",
    "        # We pass the sampled char as the next input to the model\n",
    "        # along with the hidden state from previous step\n",
    "        \n",
    "        # need to add the batch axis back before feeding to model\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        next_char = idx2char[predicted_id]\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask our model to write some poem. You can try to give it a starting word(s), e.g. Love is or Thou art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktovv0RFhrkn"
   },
   "outputs": [],
   "source": [
    "usr_input = input(\"Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: \")\n",
    "print('-'*40)\n",
    "print(generate_text(model, start_string=usr_input))\n",
    "print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try\n",
    "\n",
    "1. Train your model longer for more epochs and see if it generates better text.\n",
    "\n",
    "2. Experiment with a different start string \n",
    "\n",
    "3. Try adding another RNN layer to improve the model's accuracy\n",
    "\n",
    "4. Adjust the temperature parameter to generate more or less random predictions.\n",
    "\n",
    "5. Try replace SimpleRNN witg GRU or LSTM (to be covered in next lesson)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (tf2env)",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
