{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/master/session-8/bert-embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>\n",
    "\n",
    "# Using BERT as Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than fine-tuning BERT for downstream task such as text classification, we can use pretrained BERT model as a feature extractor, very much the same as we are using pretrained CNN such as ResNet as feature extractors for downstream task such as image classification and object detection.  \n",
    "\n",
    "In this lab, we will see how we use a pretrained DistilBert Model to extract features (or embedding) from text and use the extracted features (embeddings) to train a classifier to classify text. You can contrast this with the other lab where we train the DistilBert end to end for the classification, and compare the performance of both. \n",
    "\n",
    "At the end of this session, you will be able to:\n",
    "- prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
    "- extract text embeddings from the bert model \n",
    "- use the extracted features for text classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFAutoModel,\n",
    ")\n",
    "from transformers.utils import logging as hf_logging\n",
    "\n",
    "# We enable logging level to info and use default log handler and log formatting\n",
    "hf_logging.set_verbosity_info()\n",
    "hf_logging.enable_default_handler()\n",
    "hf_logging.enable_explicit_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the following if you have not downloaded the datasets.\n",
    "\n",
    "# !wget https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv\n",
    "# !wget https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('imdb_train.csv')\n",
    "test_df = pd.read_csv('imdb_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just use a small subset of the training and test data for this lab, as the feature extraction can take a long time, even with GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 2000\n",
    "TEST_SIZE = 200 \n",
    "\n",
    "train_df = train_df.sample(n=TRAIN_SIZE)\n",
    "test_df = test_df.sample(n=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the label to a numeric label using a simple lambda function, instead of using libraries such as LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentiment'] =  train_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['sentiment'] =  test_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We will now load the DistilBert tokenizer for the pretrained model \"distillbert-base-cased\".  This is the same as the other lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained DistilBERT [tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) expects a string or list of string, so we need to convert the data frame (or series) into list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['review'].to_list()\n",
    "train_labels = train_df['sentiment'].to_list()\n",
    "test_texts = test_df['review'].to_list()\n",
    "test_labels = test_df['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will tokenize the text string, and pad the text string to the longest sequence in the batch, and also to truncate the sequence if it exceeds the maximum length allowed by the model (in BERT's case, it is 512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
    "test_encodings = tokenizer(test_texts, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a tensorflow dataset and use it's efficient batching later to obtain the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    train_encodings['input_ids'],\n",
    "    train_labels\n",
    ")).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    test_encodings['input_ids'],\n",
    "    test_labels\n",
    ")).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_dataset.as_numpy_iterator()\n",
    "test_data = test_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate a pretrained model from 'distilbert-base-cased' and specify output_hidden_state=True so that we get the output from each of the attention layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction using (Distil)BERT. \n",
    "\n",
    "Here we will load the pretrained model for distibert-based-uncased and use it to extract features from the text (i.e. emeddings). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModel.from_pretrained(\"distilbert-base-cased\",output_hidden_states=True)\n",
    "#model = TFAutoModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will produce two outputs: the 1st output `output[0]` is of shape (16,512,768) which corresponds to the output of the last hidden layer and the second output `output[1]` is a list of 7 outputs of shape (16, 512, 718), corresponding to the output of each of the 7 attention layers. 768 refers to the hidden size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = None\n",
    "\n",
    "for batch in train_data:\n",
    "    #print(batch[0].shape)\n",
    "    output = model.predict(batch[0])\n",
    "    #print(output[0].shape)\n",
    "    hidden_states = output[1]\n",
    "    # here we take the output of the second last attention layer as our embeddings. \n",
    "    # We take the average of the embedding value of 512 tokens (at axis=1) to generate sentence embedding  \n",
    "    sentence_embeddings = tf.reduce_mean(hidden_states[-2], axis=1).numpy()\n",
    "    #print(sentence_embeddings.shape)\n",
    "    if train_embeddings is None:\n",
    "        train_embeddings = sentence_embeddings\n",
    "        #print('none')\n",
    "    else:\n",
    "        #print('got already')\n",
    "        train_embeddings = np.vstack([train_embeddings, sentence_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = None\n",
    "\n",
    "for batch in test_data:\n",
    "    output = model.predict(batch[0])\n",
    "    hidden_states = output[1]\n",
    "    # here we take the output of the second last attention layer as our embeddings. \n",
    "    # We take the average of the embedding value of 512 tokens (at axis=1) to generate sentence embedding  \n",
    "    sentence_embeddings = tf.reduce_mean(hidden_states[-2], axis=1).numpy()\n",
    "    if test_embeddings is None:\n",
    "        test_embeddings = sentence_embeddings\n",
    "    else:\n",
    "        test_embeddings = np.vstack([test_embeddings, sentence_embeddings])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_embeddings.shape)\n",
    "print(test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier using the extracted features (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_embeddings\n",
    "y_train = train_labels\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_embeddings\n",
    "y_test = test_labels\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "#clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be getting an accuracy score of around 80% which is quite good, considering we are training with only 2000 samples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Modify the code to use the output from a different attention layer as input features (embeddings) to the classifier. \n",
    "2. Try to generate sentence embeddings using different strategy, e.g. take a average of the output from multiple layers instead of only a single layer.\n",
    "2. Modify the code to use BERT model and see if it performs better than the DistilBERT. For BERT Model, the output of different layers are in `output[2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 (tf2env)",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
