{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/master/session-8/bert-finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>\n",
    "\n",
    "# Fine-tuning BERT for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the approach where we can use BERT for downstream task such as text classification is to do fine-tuning of the pretrained model. \n",
    "\n",
    "In this lab, we will see how we can use a pretrained DistilBert Model and fine-tune it with custom training data for text classification task. \n",
    "\n",
    "At the end of this session, you will be able to:\n",
    "- prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
    "- configure the transformer model for fine-tuning \n",
    "- train the model for binary and multi-class text classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments,\n",
    ")\n",
    "from transformers.utils import logging as hf_logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We enable logging level to info and use default log handler and log formatting\n",
    "hf_logging.set_verbosity_info()\n",
    "hf_logging.enable_default_handler()\n",
    "hf_logging.enable_explicit_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-17 10:20:36--  https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv\n",
      "Resolving sdaai-bucket.s3-ap-southeast-1.amazonaws.com... 52.219.36.219\n",
      "Connecting to sdaai-bucket.s3-ap-southeast-1.amazonaws.com|52.219.36.219|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13308773 (13M) [text/csv]\n",
      "Saving to: 'imdb_test.csv'\n",
      "\n",
      "imdb_test.csv       100%[===================>]  12.69M  30.2MB/s    in 0.4s    \n",
      "\n",
      "2020-12-17 10:20:37 (30.2 MB/s) - 'imdb_test.csv' saved [13308773/13308773]\n",
      "\n",
      "--2020-12-17 10:20:37--  https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv\n",
      "Resolving sdaai-bucket.s3-ap-southeast-1.amazonaws.com... 52.219.36.219\n",
      "Connecting to sdaai-bucket.s3-ap-southeast-1.amazonaws.com|52.219.36.219|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 52953551 (50M) [text/csv]\n",
      "Saving to: 'imdb_train.csv'\n",
      "\n",
      "imdb_train.csv      100%[===================>]  50.50M  41.5MB/s    in 1.2s    \n",
      "\n",
      "2020-12-17 10:20:38 (41.5 MB/s) - 'imdb_train.csv' saved [52953551/52953551]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following if you have not downloaded the datasets.\n",
    "\n",
    "!wget https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv\n",
    "!wget https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('imdb_train.csv')\n",
    "test_df = pd.read_csv('imdb_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 2500\n",
    "TEST_SIZE = 200 \n",
    "\n",
    "train_df = train_df.sample(n=TRAIN_SIZE)\n",
    "test_df = test_df.sample(n=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentiment'] =  train_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
    "test_df['sentiment'] =  test_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1278\n",
       "1    1222\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['review']\n",
    "train_labels = train_df['sentiment']\n",
    "test_texts = test_df['review']\n",
    "test_labels = test_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We will now load the DistilBert tokenizer for the pretrained model \"distillbert-base-cased\".  The tokenizer helps to produce the input tokens that are suitable to be used by the model, e.g. it automatically append the \\[CLS\\] token in the front of the sentence and the \\[SEP\\] token at the end of the token, and also the attention mask for those padded positions in the input sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:413] 2020-12-17 10:28:54,727 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/markk/.cache/torch/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "[INFO|configuration_utils.py:449] 2020-12-17 10:28:54,729 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1650] 2020-12-17 10:28:55,647 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /Users/markk/.cache/torch/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DistilBERT tokenizer (identical to Bert tokenizer) use WordPiece vocabulary. It has close to 30000 words and it maps pretrained embeddings for each. Each word has its own ids, we would need to map the tokens to those ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size = 28996\n",
      "['voices', 'shopping', '1891', 'Neil', 'discovery', '##vo', '##ations', 'burst', 'Baby', 'peaked', 'Brooklyn', 'knocked', 'lift', '##try', 'false', 'nations', 'Hugh', 'Catherine', 'preserved', 'distinguished']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer vocab size = {tokenizer.vocab_size}\")\n",
    "print(list(tokenizer.vocab.keys())[6000:6020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a closer look at the output of the tokenization process. \n",
    "\n",
    "We notice that the tokenizer will return a dictionary of two items 'input_ids' and 'attention_mask'. The input_ids contains the IDs of the tokens. While the 'attention_mask' contains the masking pattern for those padding. If you are using BERT tokenizer, there will be additional item called 'token_type_ids'.\n",
    "\n",
    "We also notice that for the example sentence, the word 'Transformer' is being broken up into two tokens 'Trans' and '##former'. Similarly, 'Processing' is tokenized as 'Process' and '##ing'.  The '##' means that the rest of the token should be attached to the previous one.\n",
    "\n",
    "We also see that the tokenizer appended \\[CLS\\] to the beginning of the token sequence, and \\[SEP\\] at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding keys:  dict_keys(['input_ids', 'attention_mask'])\n",
      "\n",
      "token ids: [101, 13809, 23763, 1110, 1541, 1363, 1111, 6240, 6828, 18821, 1158, 119, 102]\n",
      "\n",
      "tokens: ['[CLS]', 'Trans', '##former', 'is', 'really', 'good', 'for', 'Natural', 'Language', 'Process', '##ing', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Transformer is really good for Natural Language Processing.\"\n",
    "\n",
    "encoding = tokenizer(test_sentence, padding=True, truncation=True)\n",
    "print(f\"Encoding keys:  {encoding.keys()}\\n\")\n",
    "\n",
    "print(f\"token ids: {encoding['input_ids']}\\n\")\n",
    "\n",
    "print(f\"tokens: {tokenizer.convert_ids_to_tokens(encoding['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and tokenize our texts. But before we do so, we need to convert the pandas series to list first as the tokenizer cannot work with pandas series or dataframe directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_texts.to_list()\n",
    "train_labels = train_labels.to_list()\n",
    "val_texts = val_texts.to_list()\n",
    "val_labels = val_labels.to_list()\n",
    "test_texts = test_texts.to_list()\n",
    "test_labels = test_labels.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, padding=True, truncation=True)\n",
    "test_encodings = tokenizer(test_texts, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a tf dataset using the encodings and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "Now let us fine-tune our pre-trained model by training it with our custom dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first instantiate a DistilBert config object, and customise it to suit our needs. In our case, we will just specify the *num_labels* to tell the model how many labels to use in the last layer (classification layer). You only need to specify this if you are doing multi-class classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:413] 2020-12-17 10:44:28,802 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/markk/.cache/torch/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "[INFO|configuration_utils.py:449] 2020-12-17 10:44:28,803 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"distilbert-base-cased\", \n",
    "                                    num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then instantiate a DistilBert model using this config object. If the config object is not passed, the default is a binary classification. The model is a a `tf.keras.Model` subclass. So you can train the model using Keras API such as `fit()`, or use Tensorflow custom training loops if you want to have more control over the training. The transformer library however, provides a Trainer class which abstract away the complex training loop, and supports distributed training on multi-GPU system. We will use this to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the Trainer class, we need to setup the training arguments such as number of epochs, batch sizes, warming up steps (commonly used in training Transformer model), weight decay (used to by Adam Optimizer for regularization purpose), learning rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markk/anaconda3/envs/tf2env/lib/python3.8/site-packages/transformers/training_args.py:345: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluate_during_training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args_tf.py:125] 2020-12-17 10:46:34,179 >> Tensorflow: setting up strategy\n",
      "[INFO|modeling_tf_utils.py:689] 2020-12-17 10:46:35,097 >> loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/tf_model.h5 from cache at /Users/markk/.cache/torch/transformers/fe773335fbb46b412a9093627b6c3235a69c55bad3bd1deee40813cd0a8d0a82.33c483181ffc4c7cbdd0b733245bcc9b479f14f3b2e892f635fe03f4f3a41495.h5\n",
      "[WARNING|modeling_tf_utils.py:730] 2020-12-17 10:46:36,077 >> Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_tf_utils.py:742] 2020-12-17 10:46:36,077 >> Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## for distributed training on multi-gpu system, uncomment the following \n",
    "\n",
    "with training_args.strategy.scope():\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-cased\",\n",
    "        config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function `compute_metrics()`  that will be used to compute metrics at evaluation. it takes in a EvalPrediction and return a dictionary string to metric values. In our case we just return the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"acc\": (preds == p.label_ids).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer_tf.py:117] 2020-12-17 10:46:48,393 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "[INFO|trainer_tf.py:125] 2020-12-17 10:46:48,394 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n"
     ]
    }
   ],
   "source": [
    "# We define a tensorboard writer \n",
    "writer = tf.summary.create_file_writer(\"tblogs\")\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    compute_metrics = compute_metrics,\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    tb_writer=writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the training, and do the evaluation. On a single-GPU system, the training will around 6-7 minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer_tf.py:546] 2020-12-17 10:46:51,625 >> ***** Running training *****\n",
      "[INFO|trainer_tf.py:547] 2020-12-17 10:46:51,626 >>   Num examples = 2000\n",
      "[INFO|trainer_tf.py:549] 2020-12-17 10:46:51,626 >>   Num Epochs = 1\n",
      "[INFO|trainer_tf.py:550] 2020-12-17 10:46:51,627 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer_tf.py:551] 2020-12-17 10:46:51,628 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer_tf.py:554] 2020-12-17 10:46:51,629 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer_tf.py:555] 2020-12-17 10:46:51,630 >>   Steps per epoch = 125\n",
      "[INFO|trainer_tf.py:556] 2020-12-17 10:46:51,630 >>   Total optimization steps = 125\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 10:49:40,118 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 10:49:40,125 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 10:49:40,126 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 10:52:31,955 >> {'eval_loss': 0.7798129320144653, 'eval_acc': 0.4930555555555556, 'epoch': 0.08, 'step': 10}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 10:52:31,961 >> {'loss': 0.697942, 'learning_rate': 1e-06, 'epoch': 0.08, 'step': 10}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 10:55:06,059 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 10:55:06,060 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 10:55:06,061 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 10:57:54,904 >> {'eval_loss': 0.0, 'eval_acc': 0.5173611111111112, 'epoch': 0.16, 'step': 20}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 10:57:54,908 >> {'loss': 0.6947494, 'learning_rate': 2e-06, 'epoch': 0.16, 'step': 20}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 11:00:46,452 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 11:00:46,453 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 11:00:46,454 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 11:03:33,950 >> {'eval_loss': 0.0, 'eval_acc': 0.5069444444444444, 'epoch': 0.24, 'step': 30}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 11:03:33,955 >> {'loss': 0.6963409, 'learning_rate': 2.9999999e-06, 'epoch': 0.24, 'step': 30}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 11:06:05,382 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 11:06:05,383 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 11:06:05,384 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 11:08:52,369 >> {'eval_loss': 0.0, 'eval_acc': 0.5121527777777778, 'epoch': 0.32, 'step': 40}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 11:08:52,372 >> {'loss': 0.69529504, 'learning_rate': 4e-06, 'epoch': 0.32, 'step': 40}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 11:11:22,158 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 11:11:22,164 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 11:11:22,165 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 11:14:08,202 >> {'eval_loss': 0.0, 'eval_acc': 0.5225694444444444, 'epoch': 0.4, 'step': 50}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 11:14:08,205 >> {'loss': 0.69445235, 'learning_rate': 5e-06, 'epoch': 0.4, 'step': 50}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 11:16:39,862 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 11:16:39,863 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 11:16:39,863 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 11:19:26,286 >> {'eval_loss': 0.0, 'eval_acc': 0.5034722222222222, 'epoch': 0.48, 'step': 60}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 11:19:26,290 >> {'loss': 0.6929142, 'learning_rate': 5.9999998e-06, 'epoch': 0.48, 'step': 60}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 11:21:55,574 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 11:21:55,575 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 11:21:55,576 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 12:35:07,137 >> {'eval_loss': 0.0, 'eval_acc': 0.5052083333333334, 'epoch': 0.56, 'step': 70}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 12:35:07,141 >> {'loss': 0.69112676, 'learning_rate': 6.9999996e-06, 'epoch': 0.56, 'step': 70}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 12:37:57,402 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 12:37:57,403 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 12:37:57,404 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 12:40:49,955 >> {'eval_loss': 0.0, 'eval_acc': 0.5746527777777778, 'epoch': 0.64, 'step': 80}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 12:40:49,959 >> {'loss': 0.68909603, 'learning_rate': 8e-06, 'epoch': 0.64, 'step': 80}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 12:43:38,686 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 12:43:38,693 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 12:43:38,694 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 12:46:36,755 >> {'eval_loss': 0.0, 'eval_acc': 0.8194444444444444, 'epoch': 0.72, 'step': 90}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 12:46:36,758 >> {'loss': 0.68480015, 'learning_rate': 9e-06, 'epoch': 0.72, 'step': 90}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 12:49:35,736 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 12:49:35,743 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 12:49:35,744 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 12:52:31,200 >> {'eval_loss': 0.0, 'eval_acc': 0.8402777777777778, 'epoch': 0.8, 'step': 100}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 12:52:31,203 >> {'loss': 0.674449, 'learning_rate': 1e-05, 'epoch': 0.8, 'step': 100}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 12:55:20,610 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 12:55:20,611 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 12:55:20,611 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 12:58:16,904 >> {'eval_loss': 0.0, 'eval_acc': 0.8559027777777778, 'epoch': 0.88, 'step': 110}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 12:58:16,908 >> {'loss': 0.65656734, 'learning_rate': 1.1e-05, 'epoch': 0.88, 'step': 110}\n",
      "[INFO|trainer_tf.py:320] 2020-12-17 13:01:02,207 >> ***** Running Evaluation *****\n",
      "[INFO|trainer_tf.py:321] 2020-12-17 13:01:02,208 >>   Num examples = 500\n",
      "[INFO|trainer_tf.py:322] 2020-12-17 13:01:02,209 >>   Batch size = 64\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 13:03:58,776 >> {'eval_loss': 0.0, 'eval_acc': 0.8541666666666666, 'epoch': 0.96, 'step': 120}\n",
      "[INFO|trainer_tf.py:422] 2020-12-17 13:03:58,779 >> {'loss': 0.63494617, 'learning_rate': 1.19999995e-05, 'epoch': 0.96, 'step': 120}\n",
      "[INFO|trainer_tf.py:628] 2020-12-17 13:05:20,663 >> Training took: 2:18:29.026282\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs on our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from predict is logits, so we need to use a softmax to turn the values to probabilities and then use np.argmax to select the label with largest probalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictions = tf.nn.softmax(preds.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(tf_predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(preds.label_ids, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our model with our own sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"I don't see how people can sit through this hour-long movie!\"\n",
    "#test_sentence = \"This movie is in every sense flawless.\"\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"tf\")\n",
    "#labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "#print(inputs)\n",
    "out = model(inputs)\n",
    "print(np.argmax(tf.nn.softmax(out, axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "- You can try to use BERT base-cased pretrained model and see if you can get better performance. \n",
    "- Try to use BERT base-uncased pretrained model and see if you get better or worse performance.\n",
    "- You can try using a larger number of training samples. \n",
    "- Try multi-class classification using the this [dataset](https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/news.csv) that groups news title into 4 categories: e (entertainment), b (business), t (tech), m (medical/health). Original dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 (tf2env)",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
