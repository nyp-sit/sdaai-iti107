{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-pdc2-students/blob/master/iti107/session-8/nmt_attention_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Seq2Seq with Attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "In this programming exercise, we will try to improve our NMT by adding the attention mechanism. \n",
    "\n",
    "At the end of this exercise, you will be able to:\n",
    "1. understand how attention mechanism works in seq2seq \n",
    "2. implement attention mechanism in seq2seq \n",
    "\n",
    "**Note:** You need to run this in tensorflow 2.0 virtual environment.\n",
    "\n",
    "*Credit: This notebook is adapted from https://www.tensorflow.org/tutorials/text/nmt_with_attention*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "The data preparation is the same as our previous exercise on seq2seq. To avoid cluttering our notebook and to bring focus on the changes to the encoder/decoder network we made to implement attention, the data preparation codes has been moved to the `utils.py` which we need to import. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "url = 'https://sdaaidata.s3-ap-southeast-1.amazonaws.com/datasets/ind-eng.zip'\n",
    "zipfilename = 'ind-eng.zip'\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    zipfilename, origin=url,\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/ind.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = None\n",
    "src_tensor, target_tensor, src_lang_tokenizer, targ_lang_tokenizer = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_src = max_length(target_tensor), max_length(src_tensor)\n",
    "print(max_length_targ)\n",
    "print(max_length_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QILQkOs3jFG"
   },
   "outputs": [],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "src_tensor_train, src_tensor_val, target_tensor_train, target_tensor_val = train_test_split(src_tensor, target_tensor, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(src_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(src_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_src_size = len(src_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "print('src language vocab size = {}'.format(vocab_src_size))\n",
    "print('target language vocab size = {}'.format(vocab_tar_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((src_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take some sample input and target batches for use later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_src_batch, example_target_batch = next(iter(dataset))\n",
    "example_src_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Encoder and Decoder with Attention \n",
    "\n",
    "In attention model, instead of just taking in a final hidden state from the encoder network, our decoder network now infer a variable length alignment vector $a_t$ based on the current target hidden state $h_t$ (at time step $t$) and all source states $\\bar{h}_s$. A global context $c_t$ is then computed as a weighted average, according to $a_t$ over all the source states. This is shown in the diagram below:\n",
    "\n",
    "![attention layer](nb_images/attention_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder \n",
    "\n",
    "The encoder network is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        vocab_size -- vocabulary size for the embedding layer\n",
    "        embedding_size -- the length of the embedding vector\n",
    "        enc_units -- number of units in the encoder RNN layer\n",
    "        batch_sz -- batch size\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "    # Implement the forward pass\n",
    "    def call(self, sequence, hidden):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        sequence -- source sequence\n",
    "        hidden -- initial hidden state\n",
    "        \"\"\"\n",
    "        embed = self.embedding(sequence)\n",
    "        output, state = self.gru(embed, initial_state=hidden)\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_src_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "print(example_src_batch.shape)\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_src_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer \n",
    "\n",
    "Let's us implement the Attention layer using the following equations. For the scoring function, we will use the Bahdanau style (additive style). \n",
    "\n",
    "![attention layer equations](nb_images/attention_equations.png)\n",
    "\n",
    "\n",
    "Let's decide on notation before writing the pseudo-code:\n",
    "\n",
    "* FC_W1 = Fully connected (dense) layer with W1 weights\n",
    "* FC_W2 = Fully connected (dense) layer with W2 weights\n",
    "* FC_V =  Fully connected (dense) layer corresponding to $v^{\\top}_a$  \n",
    "* h_s = Encoder output (source hidden states) all time steps, shape is (batch_size, num_of_time_steps, hidden_size) \n",
    "* h_t = Decoder hidden state (or output) of each time step (hidden_size), shape is (batch_size, 1, hidden_size)\n",
    "\n",
    "*hidden_size* is the number of units in the RNN (GRU) layer\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "* `score = FC_V(tanh(FC_W1(h_t) + FC_W2(h_s)))`\n",
    "* `attention weights = softmax(score, axis = 1)` Softmax by default is applied on the last axis but here we want to apply it on the *axis=1*, since the shape of score is *(batch_size, max_length, hidden_size)* and `max_length` is the length (timesteps) of our input. Since we are trying to assign a weight to each timestep of the source sequence, softmax should be applied on that axis corresponding to timesteps, i.e. axis=1.\n",
    "* `context vector = sum(attention weights * h_s, axis = 1)`. We are summing over all time steps of source sequence, so the shape of context vector is *(batch_size, hidden_size)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Implement the equations in the Attention layer to compute the score, attention_weights and context_vector. \n",
    "\n",
    "**Hint**: use tf.reduce_sum() on axis=1 for context vector. \n",
    "\n",
    "Complete the code below. \n",
    "\n",
    "Add print statement to make sure your shape is correct.\n",
    "\n",
    "<details><summary>Click here for solution</summary>\n",
    "\n",
    "```\n",
    "score = self.FC_V(tf.nn.tanh(self.FC_W1(h_t) + self.FC_W2(h_s)))\n",
    "attention_weights = tf.nn.softmax(score, axis=1)\n",
    "context_vector = attention_weights * h_s\n",
    "context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.FC_W1 = tf.keras.layers.Dense(units)\n",
    "        self.FC_W2 = tf.keras.layers.Dense(units)\n",
    "        self.FC_V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, h_t, h_s):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        h_t -- decoder hidden state of shape (batch_size, hidden_size), i.e. one time step only\n",
    "        h_s -- encoder hidden state (output) of shape (batch_size, num_of_time_steps, hidden_size), i.e. over all time steps\n",
    "        \"\"\"\n",
    "       \n",
    "        \n",
    "        ### BEGIN YOUR CODE HERE ### \n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.FC_V\n",
    "        # the shape of the tensor before applying self.FC_V is (batch_size, max_length, units)\n",
    "        score = None\n",
    "\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = None \n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = None\n",
    "        \n",
    "        ### END YOUR CODE HERE ### \n",
    "            \n",
    "        # we add additional sequence_length (timestep) axis as axis=1 to the context vector \n",
    "        # so this is consistent to the attention output from keras implementation of the attention layers which return [batch_size, Tq, dim]\n",
    "        context_vector = tf.expand_dims(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to check your shape. You should see the following: \n",
    "```\n",
    "Attention context shape: (batch size, units) (64, 1, 1024)\n",
    "Attention weights shape: (batch_size, sequence_length, 1) (64, 29, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "print('sample_hidden shape:{}'.format(sample_hidden.shape))\n",
    "print('sample_output shape:{}'.format(sample_output.shape))\n",
    "\n",
    "sample_hidden_expanded = tf.expand_dims(sample_hidden, 1)\n",
    "print('sample_hidden expanded shape:{}'.format(sample_hidden_expanded.shape))\n",
    "attention_layer = BahdanauAttention(10)\n",
    "context, attention_weights = attention_layer(sample_hidden_expanded, sample_output)\n",
    "\n",
    "print(\"Attention context shape: (batch size, units) {}\".format(context.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder \n",
    "\n",
    "In the decoder network, at each time step _t_, we need to call the attention layer to get the context vector of time step _t_, and concatenate the _context vector_ with _hidden_state_ at time step _t_, as follows:\n",
    "\n",
    "\n",
    "![attention vector](nb_images/attention_equation2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.wc = tf.keras.layers.Dense(dec_units, activation='tanh')\n",
    "        self.uc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, sequence, state, enc_output):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        sequence -- input sequence to decoder network. In this example, we input sequence one time step at a time, so the sequence is actually of length 1 (64,1)\n",
    "        state -- in timestep 0, this is hidden state from encoder. in other timestep, this is hidden state of decoder from previous timestep (64, 1024)\n",
    "        enc_output -- output from encoder network of shape (batch_size, source_sequence_length, n_neurons), i.e. (64, 29, 1024)\n",
    "        \"\"\"\n",
    "        \n",
    "        # shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        embed = self.embedding(sequence)\n",
    "\n",
    "        # passing the embedding vector to the GRU\n",
    "        # output shape after GRU = (64, 1, 1024),  state shape after GRU = (64, 1024)\n",
    "        output, state = self.gru(embed, initial_state = state)\n",
    "\n",
    "        \n",
    "        # pass the output from decoder at timestep t, and the enc_output (of all source timesteps) to attention layer\n",
    "        # context_vector shape should be (batch_size, 1, enc_neurons)\n",
    "        # attention_weights shape should be (batch_size, enc_sequence_length,1)\n",
    "        context_vector, attention_weights = self.attention(output, enc_output)\n",
    "   \n",
    "        # We concatenate the context vector with the decoder's GRU output, and we get a concatenated vector of shape (batch_size, 2 x neurons), e.g. (64, 2048)\n",
    "        concat_out = tf.concat([tf.squeeze(context_vector, 1), tf.squeeze(output, 1)], 1)\n",
    "       \n",
    "        # We apply tanh() on concatenated output\n",
    "        # attention_vector shape should be (batch_size, wc_neurons)\n",
    "        attention_vector = self.wc(concat_out)\n",
    "        \n",
    "        # output shape == (batch_size, vocab_size)\n",
    "        logits = self.uc(attention_vector)\n",
    "        \n",
    "        return logits, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your decoder network by passing it batch of samples but with single timestep. You should expect the following output:\n",
    "\n",
    "```\n",
    "Decoder output shape: (batch_size, vocab size) (64, 3561)\n",
    "```\n",
    "\n",
    "Since our vocab size is 3561, the output is of 3561 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function and optimizer is same as the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "The training step is the same as in the previous exercise, except that now the decoder also make use of the information from the encoder through the attention layer. \n",
    "\n",
    "![encoder_decoder_with_attentions](nb_images/encoder_decoder_with_attention.png)\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n",
    "\n",
    "**Note** We are feeding the target sequence one timestep at a time to the decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        # we use the last hidden state from the encoder as the initial hidden state (t=0) to the decoder network\n",
    "        # thereafter the hidden state from previous timestep in decoder is used as input hidden state in the next timestep\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # create the input for the first timestep for decoder which is <start> token\n",
    "        # we create batch size samples of <start_token>, and shape it to <batch, 1>\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        \n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        # Note that targ.shape[1] refers the dimension of 2nd axis which is the target sequence length\n",
    "        # e.g. if target sequence is '<start> I am happy <end>', then range is range(1, 5)\n",
    "        # and t in range(1,5) will be the token at following positions: 1, 2, 3, 4\n",
    "        # i.e. I, am, happy, <end>, while dec_input is <start>, I, am, happy\n",
    "        # t is ahead of dec_input by 1 timestep\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _  = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddefjBMa3jF0"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "      # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_src))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [src_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_src,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "    print('predictions before')\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        print('predictions done')\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAM0FDomq3E"
   },
   "outputs": [],
   "source": [
    "# wrong translation\n",
    "translate(u'Apa lagi?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSx2iM36EZQZ"
   },
   "outputs": [],
   "source": [
    "# correct translation\n",
    "translate(u'Aku sudah pindah.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3LLCx3ZE0Ls"
   },
   "outputs": [],
   "source": [
    "# wrong translation\n",
    "translate(u'Aku ingin minum.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUQVLVqUE1YW"
   },
   "outputs": [],
   "source": [
    "# correct translation\n",
    "translate(u'Aku merasa lapar. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct translation\n",
    "translate(u'Terima kasih banyak.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercise\n",
    "\n",
    "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
    "* Experiment with training on a larger dataset, or using more epochs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python3 (tf2env)",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
