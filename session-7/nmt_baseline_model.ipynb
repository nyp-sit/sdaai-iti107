{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/main/session-7/nmt_baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Seq2Seq Model for Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "The most successful application of seq2seq architecture is in machine translation. We commonly use the term *Neural Machine Translation (NMT)* for neural network-based machine translation . In this week's programming exercise, we will examine a basic seq2seq architecture that consists of Encoder-Decoder pair. We will use this to translate from English to Bahasa Indonesia (simiar to Malay language). In the next programming exercise, we will modify this basic structure to include the attention mechanism to improve the translation quality.\n",
    "\n",
    "You will learn: \n",
    "1. how to implement an encoder and decoder network\n",
    "2. how a sequence to sequence model works\n",
    "3. basic processing steps in preparing text for translation\n",
    "\n",
    "*Credit: This notebook is adapted from https://www.tensorflow.org/tutorials/text/nmt_with_attention*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "\n",
    "def fix_cudnn_bug(): \n",
    "    # during training, tf will throw cudnn initialization error: failed to get convolution algos\n",
    "    # the following codes somehow fix it\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.log_device_placement = False\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "    \n",
    "fix_cudnn_bug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "what do you want to say?\tapa yang ingin kamu katakan?\n",
    "```\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-Indonesian dataset. For convenience, we've hosted a copy of this dataset on SDAAI cloud storage, but you can also download directly from the link provided above. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. Clean the sentences by removing special characters.\n",
    "2. Add a *start* and *end* token to each sentence.\n",
    "3. Convert text to numbers (vectorization) using tokenizer (tokenizer automatically creates a word index and reverse word index, i.e. dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "url = 'https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/ind-eng.zip'\n",
    "zipfilename = 'ind-eng.zip'\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    zipfilename, origin=url,\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/ind.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code converts the unicode to a normalized form so that it can be represented as ascii chars. This step is _not necessary_ for a language like Bahasa Indonesia (or Malay) as the language, like, English only contains ascii characters. But for languages like French, or German, etc, you will need to use the following code to normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
    "\n",
    "    s = s.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opI2GzOt479E"
   },
   "outputs": [],
   "source": [
    "en_sentence = u\"What do you want to say?\"\n",
    "ind_sentence = u\"Apa yang ingin kamu katakan?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(ind_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in the file contains the following fields: English sentence, Indonesian sentence and attribution. Each field is separated by a tab `(\\t)`.  For example:\n",
    "```\n",
    "It might rain tomorrow.\tHujan mungkin akan turun besok.\tCC-BY 2.0 (France) Attribution: tatoeba.org #31045 (CK) & #4449966 (Bilmanda)\n",
    "```\n",
    "We only want to keep the English and Indonesian sentence fields and drop the attribution. So we do a `line.split('\\t')` which gives us an array of 3 fields and we keep the first 2 by python slicing `[:2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. split the line into source (e.g. english), and target (e.g.indonesian) sentence fields \n",
    "# 2. normalize the source and target sentence fields \n",
    "# 3. return sentence pairs in the format: e.g. [ENGLISH, INDONESIAN]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTbSbBz55QtF"
   },
   "outputs": [],
   "source": [
    "en, ind = create_dataset(path_to_file, None)\n",
    "\n",
    "# print the last sample\n",
    "print(en[-1])\n",
    "print(ind[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmMZQpdO60dt"
   },
   "outputs": [],
   "source": [
    "def max_length(sequences):\n",
    "    return max(len(seq) for seq in sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the 'cleaned' text to a sequence of numbers (i.e. tokenize the text) and pad each sequence of numbers to the same length as our network expect all samples in the batch to be the same tensor shape. Here we use [keras Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) to do the job (there are other more advanced tokenizers such as [subwords-based tokenizer](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder) which will not be covered here). Note that we set `filters` to empty string in the Tokenizer, because we have already done our own filtering (e.g. replacing special characters with space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "    sequences = lang_tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return sequences, lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need *separate* tokenizers for source text (English) and target text (Indonesian). Here we create two tokenizers (`src_lang_tokenizer` and `targ_lang_tokenizer`), fit separately on source and target text corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    src_sentences, targ_sentences = create_dataset(path, num_examples)\n",
    "\n",
    "    src_sequences, src_lang_tokenizer = tokenize(src_sentences)\n",
    "    targ_sequences, targ_lang_tokenizer = tokenize(targ_sentences)\n",
    "\n",
    "    return src_sequences, targ_sequences, src_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "Training on the complete dataset of sentences will probably take a long time. For quick testing to see if your model is working properly (i.e. as expected, with no logic error), we can set the size of the dataset to something small (say, couple of hundred samples). If `num_samples = None`, we will use the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = None\n",
    "src_sequences, targ_sequences, src_lang_tokenizer, targ_lang_tokenizer = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_src, max_length_targ = max_length(src_sequences), max_length(targ_sequences)\n",
    "print('maximum sentence length in source text corpus = {}'.format(max_length_src))\n",
    "print('maximum sentence length in target text corpus = {}'.format(max_length_targ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "# function to print a sequence of indexes to its corresponding words\n",
    "def convert(lang_tokenizer, tokens):\n",
    "    for t in tokens:\n",
    "        if t != 0:\n",
    "            print (\"%d ----> %s\" % (t, lang_tokenizer.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXukARTDd7MT"
   },
   "outputs": [],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(src_lang_tokenizer, src_sequences[200])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang_tokenizer, targ_sequences[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "# the vocabulary size consists of all the words in the word_to_index table plus 1 reserved token of value 0\n",
    "src_vocab_size = len(src_lang_tokenizer.word_index)+1\n",
    "targ_vocab_size = len(targ_lang_tokenizer.word_index)+1\n",
    "print('src language vocab size = {}'.format(src_vocab_size))\n",
    "print('target language vocab size = {}'.format(targ_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer size for shuffling data\n",
    "BUFFER_SIZE = len(src_sequences)\n",
    "\n",
    "# batch size \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# we set the training steps per epoch to match number of batches\n",
    "steps_per_epoch = len(src_sequences)//BATCH_SIZE\n",
    "\n",
    "# this is the embedding size  \n",
    "EMBEDDING_SIZE = 256\n",
    "\n",
    "# this is the number of neuron units in the LSTM/GRU layer\n",
    "RNN_UNITS = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset\n",
    "\n",
    "We convert our training data into `tf.data.Dataset` and use it for shuffling and batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((src_sequences, targ_sequences)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if our dataset gives the correct batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Encoder and Decoder model\n",
    "\n",
    "Now let us implement the encoder and decoder network.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "First let's implement our Encoder network as shown in the dotted box: \n",
    "\n",
    "![encoder](nb_images/encoder.png)\n",
    "\n",
    "Our encoder network consists of one embedding layer, followed by a GRU layer. As we need to use the hidden state of the last timestep as the input to the decoder network, we need to set `return_state` to `True`. Although we don't really need the ouput at each timestep, we will be needing it for the attention-based model later on, so let's just set `return_sequences` to `True` also. \n",
    "\n",
    "To make our codes easier to read, we will encapsulate the details of our encoder network in a custom model by using Keras subclassing API (it is introduced in Keras 2.2.0). We just need to implement the foward pass in the `call()` method. Your `call()` needs to return both the output and final (time-step) hidden state. This final time-step encoder hidden state is to be passed as initial hidden state to the decoder.\n",
    "\n",
    "Complete the code. \n",
    "\n",
    "<details><summary>Click here for solution</summary>\n",
    "    \n",
    "```\n",
    "def __init__(...):\n",
    "    ...\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    ...\n",
    "    \n",
    "def call():\n",
    "    ... \n",
    "    output, state = self.gru(embed, initial_state = hidden)\n",
    "    ...\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        vocab_size -- vocabulary size for the embedding layer\n",
    "        embedding_size -- the length of the embedding vector\n",
    "        enc_units -- number of units in the encoder RNN layer\n",
    "        batch_sz -- batch size\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        \n",
    "        ### START YOUR CODE HERE ###\n",
    "        \n",
    "        # create an Embedding layer with appropriate size\n",
    "        self.embedding = None\n",
    "        \n",
    "        # create a gru layer with appropriate parameters. Make sure it return final output and hidden state\n",
    "        self.gru = None\n",
    "        \n",
    "        ### END YOUR CODE HERE ###\n",
    "        \n",
    "    # Implement the forward pass\n",
    "    def call(self, sequence, hidden):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        sequence -- source sequence\n",
    "        hidden -- initial hidden state\n",
    "        \"\"\"\n",
    "        \n",
    "        # call embedding layer \n",
    "        embed = self.embedding(sequence)\n",
    "        \n",
    "        ### START YOUR CODE HERE ###\n",
    "        \n",
    "        # call GRU layer and set the initial state. \n",
    "        output, state = None, None\n",
    "        \n",
    "        ### END YOUR CODE HERE \n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    # initialize encoder initial hidden state\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your encoder network by passing it the sample sequences you created from earlier notebook cell. As the sample sequences are padded to length of 38, you should expect the following output:\n",
    "\n",
    "```\n",
    "Encoder output shape: (batch size, sequence length, units) (64, 38, 1024)\n",
    "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60gSVh05Jl6l"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(src_vocab_size, EMBEDDING_SIZE, RNN_UNITS, BATCH_SIZE)\n",
    "\n",
    "print(example_input_batch.shape)\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Now let us implement our Decoder network as shown in the dotted box:\n",
    "\n",
    "![decoder.png](nb_images/decoder.png)\n",
    "\n",
    "Similar to encoder network, our decoder also consists of one embedding layer, followed by a GRU layer and a Dense layer (shown as projection layer in diagram). Our GRU needs to return output as well as hidden state at each time step, because in the training step, we will be feeding the decoder one token at a time, and compare the output at each time (step) with the expected output and calculate the loss, and pass the hidden state to the next timestep. \n",
    "\n",
    "Complete the code below.\n",
    "\n",
    "<details><summary>Click here for solution</summary>\n",
    "    \n",
    "```\n",
    "self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                               return_sequences=True,\n",
    "                               return_state=True,\n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        \n",
    "        ### START YOUR CODE HERE \n",
    "       \n",
    "    \n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def call(self, sequence, hidden):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        sequence -- target sequence (as we are using teacher forcing)\n",
    "        hidden -- hidden state (in the first timestep, the hidden state is from encoder's final hidden state)\n",
    "        \"\"\"\n",
    "        \n",
    "        # embedding shape after passing through embedding == (batch_size, 1, EMBEDDING_SIZE)\n",
    "        embed = self.embedding(sequence)\n",
    "\n",
    "        # passing the embedding to the GRU\n",
    "        output, state = self.gru(embed, hidden)\n",
    "        \n",
    "        # if one of the component of shape is -1, the size is computed automatically so that the total size is constant\n",
    "        # so if the original shape of x is (64,10,32), tf.reshape(x, (-1, 32) will become (640, 32))\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    \n",
    "        # output shape == (batch_size, vocab)\n",
    "        \n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your decoder network by passing it batch of samples but with single timestep. You should expect the following output:\n",
    "\n",
    "```\n",
    "Decoder output shape: (batch_size, vocab size) (64, 4291)\n",
    "```\n",
    "\n",
    "Since our vocab size is 4291, the output is of 4291 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(targ_vocab_size, EMBEDDING_SIZE, RNN_UNITS, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our loss function. \n",
    "\n",
    "Since we are using indexes (e.g. 23, 45, 12, etc) and not one-hot-encoded vector for our target label, we will use `SparseCategoricalCrossEntropy` as our loss function.  Note that we need to set `from_logits=True` as the output from our Decoder are logits (i.e. unscaled unnormalized values)\n",
    "\n",
    "As our sequences are padded with 0 (to be the same length), we don't want to take these zeros into account when computing the loss. One way to do this is to compute the mask and use the mask to zero out the loss at those positions that are padded. See the diagram here:\n",
    "\n",
    "![mask](nb_images/mask_loss.png)\n",
    "\n",
    "You can first compare each position to zero by using `tf.math.equal()`. This will set to True for those positions that are zeros. You can then invert that using `tf.math.logical_not()` so that your final mask will be True for those non-zero positions. \n",
    "\n",
    "You can then use the mask to do element-wise multiplication with the loss. But before you can do that you need to cast the mask (which are of boolean type) to whatever dtype the loss is by using `tf.cast(x, dtype=loss.dtype)`\n",
    "\n",
    "**Exercise:** \n",
    "\n",
    "Complete the code in `loss_func()`. \n",
    "\n",
    "<details><summary>Click here for solution</summary>\n",
    "    \n",
    "```\n",
    "\n",
    "mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "loss_ = loss_object(real, pred)\n",
    "\n",
    "mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "loss_ *= mask\n",
    "\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "    \n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "The following diagram shows the training process using teaching forcing:\n",
    "\n",
    "![training seq2seq](nb_images/seq2seq_train.png)\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The *decoder hidden state* is passed to the model in the next timestep. The *prediction* is compared with expected to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n",
    "\n",
    "**Note** We are feeding the target sequence one timestep at a time to the decoder\n",
    "\n",
    "*In the code below, you will see the use @tf.function at the beginning of the function. It basically transforms the python function into a high-performing tensorflow graph for performance reason*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(src, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # create the input for the first timestep for decoder which is <start> token\n",
    "        # we create batch size samples of <start_token>, and shape it to <batch, 1>\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        # Note that targ.shape[1] refers the dimension of 2nd axis which is the target sequence length\n",
    "        # e.g. if target sequence is '<start> I am happy <end>', then  range(1, 5)\n",
    "        # will be the tokens at following positions: 1, 2, 3, 4\n",
    "        # i.e. 'I', 'am', 'happy', ''<end>', while dec_input is <start>, I, am, happy\n",
    "        # t is ahead of dec_input by 1 timestep\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # we advance the input to the next timestep\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddefjBMa3jF0"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "train = False \n",
    "\n",
    "if train:\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (src, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(src, targ, enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "          # saving (checkpoint) the model every 2 epochs\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                          total_loss / steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the previous hidden state. For timestep 0, the hidden state of decoder is set to the hidden state of the last timestep of encoder.\n",
    "* Stop predicting when the model predicts the *end token \\<end\\>* .\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    \n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [src_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_src,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, RNN_UNITS))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden = decoder(dec_input,dec_hidden)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the following if you want to download the pretrained model checkpoints \n",
    "# !wget https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/pretrained-weights/iti107/session-8/nmt-chk-30epochs.tar.gz\n",
    "# !tar xvf nmt-chk-30epochs.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = [\n",
    "    \"We hope prices are going to drop.\",\n",
    "    \"You look familiar. Do I know you?\",\n",
    "    \"I went to see a doctor.\", \n",
    "    \"I'm sorry, but I'm busy right now.\",\n",
    "    \"I have moved out.\",\n",
    "    \"there was a heavy rain this morning.\",\n",
    "    \"My wife likes the painting.\",\n",
    "    \"I ate two slices of bread.\",\n",
    "    \"I can't go out because I broke my leg.\",\n",
    "    \"This is a very cold morning.\"\n",
    "]\n",
    "\n",
    "for sent in test_sents: \n",
    "    translate(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTe5P5ioMJwN"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* As we can see, our baseline model got some of the translations correct, but in some cases, the translations made no sense at all. We will try to improve the model using Attention in our next exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (tf2env)",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
