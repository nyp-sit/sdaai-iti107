{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/main/session-4/object_detection_yolov2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with YOLOv2\n",
    "\n",
    "Welcome to this week's object detection programming exercise. This exercise allows you to implement some of the main ideas we covered in the lecture for YOLOv2 algorithm, such as anchor boxes, IoU and non-max-suppression. \n",
    "\n",
    "**You will learn how to:**\n",
    "- implement IoU, Non-Max-Suppression \n",
    "- apply object detection on image and video\n",
    "\n",
    "**Note**: No GPU is required to run this notebook because we are not training any model :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:18:52.056478",
     "start_time": "2018-04-04T00:18:50.879887"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import imgaug as ia\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from darknet import build_darknet\n",
    "from utils import sigmoid, softmax, draw_boxes\n",
    "from utils import fix_cudnn_bug\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Build he YOLO network (Darknet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:18:53.978220",
     "start_time": "2018-04-04T00:18:53.967537"
    }
   },
   "outputs": [],
   "source": [
    "model = build_darknet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "source": [
    "**Exercise**: \n",
    "\n",
    "Examine the model using `model.summary()`.\n",
    "\n",
    "1. What is the input shape (of the image) for the YOLO's Darknet? \n",
    "\n",
    "2. What is the output shape of the the YOLO's Darknet\n",
    "\n",
    "3. What does axis 2 and 3 of the YOLO output represent respectively? \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "1. (416, 416, 3)\n",
    "2. (13, 13, 5, 85)\n",
    "3. Axis 2 (number 5) represents the 5 bounding boxes. Axis 3 (number 85) represents the confidence_score, x,y,w,h of the bounding box, and 80 class probabalities.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained weights for YOLOv2 converted to .h5 format (pre-trained on COCO dataset) can be downloaded from: \n",
    "\n",
    "https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/pretrained-weights/iti107/session-4/full_yolov2.h5\n",
    "\n",
    "The original YOLO pretrained weights and config file can be downloaded from YOLO's author website: \n",
    "\n",
    "https://pjreddie.com/media/files/yolov2.weights\n",
    "\n",
    "However this weight file need to be converted to .h5 format before it can be loaded using keras's ```model.load_weights()``` method. \n",
    "\n",
    "Refer to this link on how to convert the weights:\n",
    "\n",
    "https://github.com/allanzelener/YAD2K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this only if you have wget installed on your Linux system. Otherwise just download it using your browser \n",
    "# and place this file in the same directory as this ipython notebook\n",
    "! wget https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/pretrained-weights/iti107/session-4/full_yolov2.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"full_yolov2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process the detection output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define below some configuration parameters used by the YOLOv2. \n",
    "\n",
    "![anchor_boxes](nb_images/anchor_boxes.png)\n",
    "- **Anchor boxes** (`ANCHOR_BOXES`) YOLOv2 uses 5 anchor boxes to detect multiple objects within a grid cell.  The 10 numbers in `ANCHOR_BOXES` below represents pairs of (width and height) of the 5 different anchor boxes. The dimension is relative to a 13 x 13 grid cell.\n",
    "- **Object Threshold** (`OBJ_THRESHOLDS`) controls which boxes to keep based on confidence score and class probabilities of each predicted bounding box\n",
    "- **NMS threshold** (`NMS_THRESHOLD`) is the IoU threshold used to decide whether to remove a bounding box in the case of multiple predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:18:52.075535",
     "start_time": "2018-04-04T00:18:52.057712"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 80\n",
    "OBJ_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.5\n",
    "ANCHOR_BOXES = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the class labels from the file labels.txt (this will contain all the names for the 80 classes). Open the file (the file is found in the same directory as this notebook) to examine the names of the classes the YOLO detector will detect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the class labels YOLOv2 is trained on\n",
    "labels = [label.rstrip('\\n') for label in open('labels.txt')]\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounding box\n",
    "\n",
    "Here we define a convenient class to hold the information about each bounding box predicted (e.g. xmin, ymin, xmax, ymax, representing 4 corners of the rectangle box, confidence score ('object-ness') and the class probabilities of the object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, c=None, class_prob_score=None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "            \n",
    "        # This is the confidence score\n",
    "        self.confidence = c\n",
    "        # This is the class probabalities score (confidence score * class probabilities)\n",
    "        self.class_prob_scores = class_prob_score\n",
    "\n",
    "        self.label = -1\n",
    "        self.class_prob_score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            # return the class label corresponding to the highest class probability score\n",
    "            self.label = np.argmax(self.class_prob_scores)\n",
    "\n",
    "        return self.label\n",
    "\n",
    "    def get_score(self):\n",
    "        if self.class_prob_score == -1:\n",
    "            self.class_prob_score = self.class_prob_scores[self.get_label()]\n",
    "\n",
    "        return self.class_prob_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "Given a tensor (`netout`) of the output shape above, and given a object threshold  (`obj_threshold`), we want to filter out those boxes that is low in class-specific confidence scores, i.e. lower than the `obj_threshold`. These are the steps you need:\n",
    "\n",
    "1. Retrieve the box confidence from the netout and apply sigmoid function `sigmoid(x)` so that the confidence score is between 0 and 1. (`sigmoid()` is already imported)\n",
    "2. Retrieve the box class probabilities (80 of them) and apply function `softmax(x)`  so that the class probabilities sums to 1.0 (`softmax()` already imported)\n",
    "3. Compute the scores of class probabilities by multiplying (element-wise) the box confidence score with class probabalities (see the diagram below for clearer picture of how this is done, box confidence score is denoted as $p_c$ in the diagram)\n",
    "4. For those with scores (from step 3) < `obj_threshold`, set box score to 0, so that the associated box will be removed later\n",
    "\n",
    "<img src=\"nb_images/probability_extraction.png\" style=\"width:500px;height:400;\"/>\n",
    "\n",
    "***Hint***:\n",
    "\n",
    "The output of the detection layer (which we call `netout` in the later code cell) is of the shape 13 x 13 x 5 x 85\n",
    "\n",
    "It'll be convenient to rearrange the (13,13,5,85) tensor to retrieve the following information:  \n",
    "- `box_confidence`: tensor of shape $(13 \\times 13, 5, 1)$ containing $p_c$ (confidence probability that there's some object) for each of the 5 boxes predicted in each of the 19x19 cells.\n",
    "- `boxes`: tensor of shape $(13 \\times 13, 5, 4)$ containing $(b_x, b_y, b_h, b_w)$ for each of the 5 boxes per cell.\n",
    "- `box_class_probs`: tensor of shape $(13 \\times 13, 5, 80)$ containing the detection probabilities $(c_1, c_2, ... c_{80})$ for each of the 80 classes for each of the 5 boxes per cell.\n",
    "\n",
    "\n",
    "To quickly access the elements of the last axis (i.e. the axis that has 85 elements), you can use the ellipsis in numpy array to expand to the number of ':' objects needed to make a selection tuple of the same length as x.ndim. \n",
    "\n",
    "e.g. Given an numpy array x of shape (2,3,2) (so ndim = 3):\n",
    "```\n",
    "[[[ 0  1]\n",
    "  [ 2  3]\n",
    "  [ 4  5]]\n",
    "\n",
    " [[ 6  7]\n",
    "  [ 8  9]\n",
    "  [10 11]]]\n",
    "```\n",
    "``x[..., 0]`` will give  ``[[0,2,4], [6,8,10]]`` and ``x[..., 1]`` will give ``[[1,3,5], [7,9,11]]``\n",
    "\n",
    "\n",
    "\n",
    "***This is quite a challenging exercise. Don't worry if you do not know how to do it :)***\n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "```\n",
    "def filter_boxes(netout, obj_threshold=0.3):\n",
    "    \n",
    "    boxes = []\n",
    "    \n",
    "    # Step 1\n",
    "    box_confidences = netout[..., 4] \n",
    "    box_confidences = sigmoid(box_confidences)\n",
    "\n",
    "    # make the box_confidences the same number of axis as box_class_probs so you can multiply them together\n",
    "    box_confidences = box_confidences[..., np.newaxis]\n",
    "    \n",
    "    # Step 2\n",
    "    box_class_probs = netout[..., 5:]   # 5th element onwards are individual class probs\n",
    "    box_class_probs = softmax(box_class_probs)\n",
    "    \n",
    "    # Step 3\n",
    "    # Compute box clas prob scores by doing the elementwise product of box_confidences and box_class_probs\n",
    "    # You need both box_confidnences and box_class_probs to have the same number of axis\n",
    "    box_scores = box_confidences * box_class_probs\n",
    "    \n",
    "    # Step 4\n",
    "    # for class probablies less than threshold, set it to 0 other set it to 1\n",
    "    box_scores *= box_scores > obj_threshold\n",
    "    \n",
    "    return box_confidences, box_class_probs, box_scores\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(netout, obj_threshold=0.3):\n",
    "    \n",
    "    boxes = []\n",
    "    \n",
    "    # look at the last axis which is the one with 85 elements\n",
    "    # out of these 85, 5 is (x,y, w, h, confidence)\n",
    "    # element at index 4 (5th element) is the confidence score\n",
    "    \n",
    "    ### STEP 1: START YOUR CODE HERE ###\n",
    "    \n",
    "    box_confidences = None\n",
    "   \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # make the box_confidences the same number of axis as box_class_probs so you can multiply them together\n",
    "    box_confidences = box_confidences[..., np.newaxis]\n",
    "    \n",
    "    ### STEP 2: START YOUR CODE HERE ###\n",
    "    \n",
    "    # The last axis's 5th element onwards are individual class probs\n",
    "    box_class_probs = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "   \n",
    "    ### STEP 3: START YOUR CODE HERE ###\n",
    "    \n",
    "    # Compute box class prob scores by doing the elementwise product of box_confidences and box_class_probs\n",
    "    # You need both box_confidences and box_class_probs to have the same number of axis\n",
    "    box_scores = None\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    ### STEP 4: START YOUR CODE HERE ###\n",
    "    \n",
    "    # for class probabilities less than threshold, set it to 0 other set it to 1\n",
    "    box_scores = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return box_confidences, box_class_probs, box_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to check the shapes of different outputs from filter_boxes(). You should expect the shape to be following:\n",
    "\n",
    "```\n",
    "box_confidences shape = (13, 13, 5, 1)\n",
    "box_class_probs shape = (13, 13, 5, 80)\n",
    "box_scores shape = (13, 13, 5, 80)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if your filter boxes function returns the correct shape \n",
    "sample_netout = np.random.rand(13, 13, 5, 85)\n",
    "box_conf, box_cls_probs, box_scores = filter_boxes(sample_netout, OBJ_THRESHOLD)\n",
    "print('box_confidences shape = {}'.format(box_conf.shape))\n",
    "print('box_class_probs shape = {}'.format(box_cls_probs.shape))\n",
    "print('box_scores shape = {}'.format(box_scores.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding all the bounding boxes that has high box scores\n",
    "\n",
    "The following codes loops through each of the 13 x 13 grid cells, and for each of the 5 bounding boxes of each grid cell that has box_score > 0 (as set by filter_boxes function above, indicating there is an object of certain class detected), compute the bounding box's x_min, y_min (top left corner) and x_max, y_max (bottom left corner) using the following formula (reproduced from YOLOv2 paper), where $p_w$ and $p_h$ are the width and height of the corresponding anchor box, and $t_x$, $t_y$, $t_w$ and $t_h$ are the 4 coordinates of each bounding box, and $c_x$, $c_y$ is the offset from top-left corner of the image (corresponds to grid location):\n",
    "\n",
    "<img src=\"nb_images/bounding_box_equations.png\" style=\"width:150px;height:100\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_netout(netout, anchors, nb_class, obj_threshold=0.3, nms_threshold=0.3):\n",
    "    boxes = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # call the filter_box() to get box_confidences, box_class_probs and box_scores\n",
    "    box_confidences, box_class_probs, box_scores = filter_boxes(netout, obj_threshold)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grid_h, grid_w, nb_box = netout.shape[:3]\n",
    "    \n",
    "    # calculate the locations of each of the 5 bounding boxes for each of the 13 x 13 locations\n",
    "    count = 0\n",
    "    for row in range(grid_h):\n",
    "        for col in range(grid_w):\n",
    "            for b in range(nb_box):\n",
    "                # from 4th element onwards are confidence and class classes\n",
    "                box_score = box_scores[row,col,b]\n",
    "                \n",
    "                # if scores for all classes are 0, then skip the box\n",
    "                if np.sum(box_score) > 0:\n",
    "                    # first 4 elements are x, y, w, and h\n",
    "                    x, y, w, h = netout[row,col,b,:4]\n",
    "            \n",
    "                    # x that is output is relative to each cell, so need to compute the \n",
    "                    # x, and y is the coordinate of the center of the bounding \n",
    "                    x = (sigmoid(x) + col) / grid_w # center position, unit: image width\n",
    "                    y = (sigmoid(y) + row) / grid_h # center position, unit: image height\n",
    "                    w = anchors[2 * b + 0] * np.exp(w) / grid_w # unit: image width\n",
    "                    h = anchors[2 * b + 1] * np.exp(h) / grid_h # unit: image height\n",
    "                    \n",
    "                    confidence = box_confidences[row,col,b]\n",
    "                    \n",
    "                    # convert the coordinate to top/left corner and bottom/right corner \n",
    "                    x_min = x - w/2\n",
    "                    x_max = x + w/2 \n",
    "                    y_min = y - h/2\n",
    "                    y_max = y + h/2 \n",
    "                    \n",
    "                    box = BoundBox(x_min, y_min, x_max, y_max, confidence, box_score)\n",
    "\n",
    "                    boxes.append(box)\n",
    "                    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union\n",
    "\n",
    "Non-max suppression uses a very important function called **\"Intersection over Union\"**, or IoU.\n",
    "<img src=\"nb_images/iou.png\" style=\"width:500px;height:400;\"/>\n",
    "<caption><center> Definition of \"Intersection over Union\"<br> </center></caption>\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "Implement bbox_iou(). \n",
    "\n",
    "Some hints:\n",
    "- In this exercise only, we define a box using its two corners (upper left and lower right): `(xmin, ymin, xmax, ymax)` rather than the midpoint and height/width.\n",
    "- To calculate the area of a rectangle you need to multiply its height `(ymax - ymin)` by its width `(xmax - xmin)`.\n",
    "- You'll also need to find the coordinates `(x1_i, y1_i, x2_i, y2_i)` of the intersection of two boxes. \n",
    "\n",
    "Remember that:\n",
    "- x1_i = maximum of the x1 coordinates of the two boxes\n",
    "- y1_i = maximum of the y1 coordinates of the two boxes\n",
    "- x2_i = minimum of the x2 coordinates of the two boxes\n",
    "- y2_i = minimum of the y2 coordinates of the two boxes\n",
    "\n",
    "In order to compute the intersection area, you need to make sure the height and width of the intersection are positive, otherwise the intersection area should be zero. You can use `max(height, 0)` and `max(width, 0)` to ensure that the values are always positive.\n",
    "\n",
    "In this code, we use the convention that (0,0) is the top-left corner of an image, (1,0) is the upper-right corner, and (1,1) the lower-right corner. \n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "<br/>\n",
    "    \n",
    "```\n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"Implement the intersection over union (IoU) between box1 and box2\n",
    "    \n",
    "    Arguments:\n",
    "    box1 -- first box, which is an object with the following attributes(xmin, ymin, xmax, ymax)\n",
    "    box2 -- second box, which is an object with the following attributes(xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the intersection\n",
    "    x1_i = max(box1.xmin, box2.xmin)  \n",
    "    y1_i = max(box1.ymin, box2.ymin)\n",
    "    x2_i = min(box1.xmax, box2.xmax)\n",
    "    y2_i = min(box1.ymax, box2.ymax)\n",
    "    intersection_w = max(x2_i - x1_i, 0)\n",
    "    intersection_h = max(y2_i - y1_i, 0)\n",
    "    intersection_area = intersection_w * intersection_h\n",
    "    \n",
    "    # calculate the union \n",
    "    box1_area = (box1.xmax - box1.xmin) * (box1.ymax - box1.ymin)\n",
    "    box2_area = (box2.xmax - box2.xmin) * (box2.ymax - box2.ymin)\n",
    "    \n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    # calculate the iou \n",
    "    iou = float(intersection_area)/union_area\n",
    "    \n",
    "    return iou\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"Implement the intersection over union (IoU) between box1 and box2\n",
    "    Arguments:\n",
    "    box1 -- first box, which is an object with the following attributes(xmin, ymin, xmax, ymax)\n",
    "    box2 -- second box, which is an object with the following attributes(xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    \n",
    "    # Step 1: calculate the intersection\n",
    "    x1_i = None\n",
    "    y1_i = None\n",
    "    x2_i = None\n",
    "    y2_i = None\n",
    "    intersection_w = max(x2_i - x1_i, 0)\n",
    "    intersection_h = max(y2_i - y1_i, 0)\n",
    "    intersection_area = intersection_w * intersection_h\n",
    "    \n",
    "    # Step 2: calculate the union \n",
    "    \n",
    "    box1_area = None\n",
    "    box2_area = None\n",
    "\n",
    "    union_area = None\n",
    "\n",
    "    # Step 3: calculate the iou\n",
    "    \n",
    "    iou = None\n",
    "    \n",
    "    ### END YOUR CODE HERE ###\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Max Suppression\n",
    "\n",
    "Here is the code that implements non-max suppression. The key steps are: \n",
    "1. Select the box that has the highest score.\n",
    "2. Compute the overlap of this box with all other boxes, and remove boxes that overlap significantly (`iou >= nms_threshold`).\n",
    "3. Go back to step 1 and iterate until there are no more boxes with a lower score than the currently selected box.\n",
    "\n",
    "This will remove all boxes that have a large overlap with the selected boxes. Only the \"best\" boxes remain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, nb_class, nms_threshold, obj_threshold):\n",
    "\n",
    "    # np.argsort sorts in ascending order, we reverse so we will look at box with highest probabalities\n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = list(reversed(np.argsort([box.class_prob_scores[c] for box in boxes])))\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "            \n",
    "            if boxes[index_i].class_prob_scores[c] == 0: \n",
    "                continue\n",
    "            else:\n",
    "                for j in range(i+1, len(sorted_indices)):\n",
    "                    index_j = sorted_indices[j]\n",
    "                    \n",
    "                    if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_threshold:\n",
    "                        boxes[index_j].class_prob_scores[c] = 0\n",
    "                        \n",
    "    # remove the boxes which are less likely than obj_threshold\n",
    "    boxes = [box for box in boxes if box.get_score() > 0]\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform detection on image\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Before you give the image to the model, it needs to be preprocessed as follows:\n",
    "\n",
    "1. Resize the image to the correct input size expected by the model (recall the input size from Part 2)\n",
    "2. Normalize values of each pixel to between (0,1)\n",
    "3. Reverse the channel order (if necessary) \n",
    "\n",
    "**Note**: opencv reads in image using BGR ordering, so we need to reverse it. ***Hint***:  use ``::-1`` to reverse the items\n",
    "4. Add additional dimension as 1st dimension as the model expects to receive inputs in batches, i.e of shape (batch, width, height, channels).  ***Hint*** use ```np.expand_dims()```\n",
    "5. Call `model.predict()` to get the predictions of shape (13, 13, 5, 85)\n",
    "6. Pass the predictions to `decode_netout()` to get the bounding boxes.  ***Hint*** remove the 1st axis (batch axis) before call decode_netout()\n",
    "7. Call `non_max_suppression()` to remove duplicate predictions.\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```\n",
    "input_image = cv2.resize(image, (416, 416))\n",
    "input_image = input_image / 255.\n",
    "input_image = input_image[:,:,::-1]\n",
    "input_image = np.expand_dims(input_image, 0)\n",
    "\n",
    "netout = model.predict(input_image)\n",
    "\n",
    "boxes = decode_netout(netout[0], \n",
    "                      anchors=ANCHOR_BOXES, \n",
    "                      nb_class=NUM_CLASSES)\n",
    "\n",
    "boxes = non_max_suppression(boxes, NUM_CLASSES, NMS_THRESHOLD, OBJ_THRESHOLD)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:19:07.263359",
     "start_time": "2018-04-04T00:19:05.658285"
    }
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('data/person.jpg')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "### END THE CODE  ###\n",
    "\n",
    "## draw the box on the original image, not preprocessed image\n",
    "image = draw_boxes(image, boxes, labels=labels)\n",
    "\n",
    "## reverse the BGR to RGB channel ordering\n",
    "plt.imshow(image[:,:,::-1]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform detection on video\n",
    "\n",
    "The following code shows how to perform detection on video and write the result (image with drawn bounding boxes) to an image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-06T13:39:09.640646Z",
     "start_time": "2017-10-06T13:31:44.627609Z"
    }
   },
   "outputs": [],
   "source": [
    "video_inp = 'data/street.mp4'\n",
    "video_out = 'data/street_predicted.mp4'\n",
    "\n",
    "video_reader = cv2.VideoCapture(video_inp)\n",
    "\n",
    "nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "video_writer = cv2.VideoWriter(video_out,\n",
    "                               cv2.VideoWriter_fourcc(*'XVID'), \n",
    "                               30.0, \n",
    "                               (frame_w, frame_h))\n",
    "\n",
    "for i in tqdm(range(nb_frames)):\n",
    "    ret, image = video_reader.read()\n",
    "    \n",
    "    input_image = cv2.resize(image, (416, 416))\n",
    "    input_image = input_image / 255.\n",
    "    input_image = input_image[:,:,::-1]\n",
    "    input_image = np.expand_dims(input_image, 0)\n",
    "\n",
    "    netout = model.predict(input_image)\n",
    "\n",
    "    boxes = decode_netout(netout[0], \n",
    "                          obj_threshold=0.3,\n",
    "                          nms_threshold=NMS_THRESHOLD,\n",
    "                          anchors=ANCHOR_BOXES, \n",
    "                          nb_class=NUM_CLASSES)\n",
    "    image = draw_boxes(image, boxes, labels=labels)\n",
    "\n",
    "    video_writer.write(np.uint8(image))\n",
    "    \n",
    "video_reader.release()\n",
    "video_writer.release()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's playback the video that we have created. \n",
    "\n",
    "***Note***: \n",
    "\n",
    "Only run the cell below if if you are running on a local PC. The opencv needs to open a local window to play the video and this is not possible if you remotely access the server through a browser (e.g. when you are using cloud VM). So, if you are using the cloud VM, you can download the video to your local PC and play it using any video player. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open the video file and play it\n",
    "video_out = 'data/street_predicted.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_out)\n",
    "\n",
    "if (cap.isOpened() == False):\n",
    "    print('Error')\n",
    "\n",
    "cv2.namedWindow('Frame')\n",
    "cv2.startWindowThread()    \n",
    "while(cap.isOpened()):   \n",
    "    ret, frame = cap.read() \n",
    "    cv2.startWindowThread()\n",
    "    if ret == True: \n",
    "        # Display the resulting frame \n",
    "        cv2.imshow('Frame', frame) \n",
    "\n",
    "        # Press Q on keyboard to  exit \n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.waitKey(5000) # 5 sec delay before image window closes\n",
    "cv2.destroyWindow(\"Frame\")\n",
    "cv2.waitKey(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Exercise**\n",
    "\n",
    "Try using your own image or video to do Object Detection, have FUN!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "tf2env",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "nav_menu": {
    "height": "122px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "758px",
    "left": "0px",
    "right": "1096px",
    "top": "73px",
    "width": "253px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
